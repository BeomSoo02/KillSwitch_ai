# app.py â€” KillSwitch AI (ë¶€ìŠ¤ ì „ì‹œìš©, GPT í•­ìƒ í‘œì‹œ)
# - ì „ì‹œìš©: í° íŒì • ì¹´ë“œ + ê°„ë‹¨ ì´ìœ 
# - GPT ì‘ë‹µ: ì „ë¬¸ê°€ ëª¨ë“œì™€ ë¬´ê´€í•˜ê²Œ í•­ìƒ ì„¹ì…˜ í‘œì‹œ (Key/í—ˆìš© ì‹œ ì¦‰ì‹œ í˜¸ì¶œ)
# - ì „ë¬¸ê°€ ëª¨ë“œ: ì„¸ë¶€ ì§€í‘œ/JSONë§Œ í† ê¸€
# --------------------------------------------------------------------------------------------
import os, re, time, unicodedata
import streamlit as st

st.set_page_config(page_title="KillSwitch AI", layout="wide")

# ===== 1) í™˜ê²½/ì‹œí¬ë¦¿ =====
DEFAULT_REPO_ID   = "cookiechips/KillSwitch_ai"   # ê¸°ë³¸ê°’: ê³µê°œ ë¦¬í¬
DEFAULT_REPO_TYPE = "model"
DEFAULT_FILENAME  = "prompt_guard_best.pt"

REPO_ID   = st.secrets.get("HF_REPO_ID")   or os.getenv("HF_REPO_ID")   or DEFAULT_REPO_ID
REPO_TYPE = st.secrets.get("HF_REPO_TYPE") or os.getenv("HF_REPO_TYPE") or DEFAULT_REPO_TYPE
FILENAME  = st.secrets.get("HF_FILENAME")  or os.getenv("HF_FILENAME")  or DEFAULT_FILENAME
HF_TOKEN  = st.secrets.get("HF_TOKEN")     or os.getenv("HF_TOKEN")     # ë¹„ê³µê°œ ë¦¬í¬ë©´ í•„ìš”
HF_DIR    = st.secrets.get("HF_DIR")       or os.getenv("HF_DIR")       # ì™„ì „ ëª¨ë¸ ë””ë ‰í† ë¦¬(ì˜µì…˜)

BASE_MODEL = os.getenv("BASE_MODEL") or "microsoft/deberta-v3-base"
NUM_LABELS = int(os.getenv("NUM_LABELS") or 2)

# ë©”íƒ€ ê°€ì¤‘ì¹˜
W_ACTION   = 0.15   # ì‹¤í–‰í˜•(ê°•í•œ action)
W_INFO     = -0.15  # ì„¤ëª…í˜•(info) â€” ìˆœìˆ˜ ì •ë³´ì„±ì¼ìˆ˜ë¡ ê°ì‚°
W_DOMAIN   = 0.25   # ìœ„í—˜ ë‹¨ì–´(domain risk)
GAP_THR    = 0.10   # softmax í™•ì‹  ì„ê³„ê°’
W_UNCERT   = -0.10  # gap < 0.10ì´ë©´ ë¶ˆí™•ì‹¤ â†’ ê°ì‡ 

# ===== 2) ëª¨ë¸/í—ˆë¸Œ ë¡œë”© =====
import torch
from huggingface_hub import hf_hub_download
from transformers import AutoTokenizer, AutoModelForSequenceClassification

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

@st.cache_resource(show_spinner=False)
def get_ckpt_path() -> str:
    """í—ˆë¸Œì—ì„œ .pt ì²´í¬í¬ì¸íŠ¸ ë‹¤ìš´ë¡œë“œ (ê¸°ë³¸ repo_type ì‹¤íŒ¨ ì‹œ ë°˜ëŒ€ íƒ€ì…ë„ ì‹œë„)."""
    local = os.path.join("model", FILENAME)
    if os.path.exists(local):
        return local
    try:
        return hf_hub_download(repo_id=REPO_ID, filename=FILENAME, repo_type=REPO_TYPE, token=HF_TOKEN)
    except Exception:
        alt = "dataset" if REPO_TYPE == "model" else "model"
        return hf_hub_download(repo_id=REPO_ID, filename=FILENAME, repo_type=alt, token=HF_TOKEN)

@st.cache_resource(show_spinner=False)
def load_model_tokenizer():
    """
    1) HF_DIR ìˆìœ¼ë©´ ì™„ì „ ëª¨ë¸ ë””ë ‰í† ë¦¬ì—ì„œ from_pretrained
    2) ì•„ë‹ˆë©´ BASE_MODEL ë¡œë“œ í›„ .pt state_dict ì£¼ì…
    - PyTorch 2.6+ í˜¸í™˜: torch.load(..., weights_only=False) ëª…ì‹œ
    """
    # í† í¬ë‚˜ì´ì €: slow ìš°ì„  â†’ ì‹¤íŒ¨ ì‹œ fast
    try:
        tok = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=False)
        tok_info = "slow"
    except Exception:
        tok = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)
        tok_info = "fast"

    # 1) ì™„ì „ ëª¨ë¸ ë””ë ‰í† ë¦¬
    if HF_DIR and os.path.isdir(HF_DIR):
        mdl = AutoModelForSequenceClassification.from_pretrained(HF_DIR)
        mdl.to(DEVICE).eval()
        return mdl, tok, 0.5, True, tok_info

    # 2) ë² ì´ìŠ¤ + state_dict
    mdl = AutoModelForSequenceClassification.from_pretrained(BASE_MODEL, num_labels=NUM_LABELS)
    thr = 0.5
    torch_loaded = False
    try:
        ckpt_path = get_ckpt_path()
        # âš  PyTorch 2.6 ê¸°ë³¸ê°’ ë³€ê²½ ëŒ€ì‘: weights_only=False ë¡œ ëª…ì‹œ
        ckpt = torch.load(ckpt_path, map_location="cpu", weights_only=False)
        state = ckpt["model"] if isinstance(ckpt, dict) and "model" in ckpt else ckpt

        missing, unexpected = mdl.load_state_dict(state, strict=False)
        if missing or unexpected:
            st.caption(f"state_dict mismatch â†’ missing:{len(missing)}, unexpected:{len(unexpected)}")

        # ì²´í¬í¬ì¸íŠ¸ì— ì„ê³„ê°’ì´ ì €ì¥ë¼ ìˆìœ¼ë©´ ì‚¬ìš© (ì„ íƒ)
        if isinstance(ckpt, dict) and "val_thr" in ckpt:
            try:
                thr = float(ckpt["val_thr"])
            except Exception:
                pass

        torch_loaded = True
    except Exception as e:
        st.error("ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì‹¤íŒ¨ â€” ëª¨ë¸ ë¯¸ë¡œë”©")
    mdl.to(DEVICE).eval()
    return mdl, tok, thr, torch_loaded, tok_info

_cached_model = load_model_tokenizer

# ===== 3) ì „ì²˜ë¦¬ & ìŠ¤ì½”ì–´ë§ =====
def preprocess(s: str) -> str:
    """NFKC ì •ê·œí™” + ê³µë°± ì •ë¦¬ + ëë§ˆì¹¨í‘œ ê°•ì œ(ì´ë¯¸ ìˆìœ¼ë©´ ì¶”ê°€í•˜ì§€ ì•ŠìŒ)."""
    s = unicodedata.normalize("NFKC", s or "")
    s = re.sub(r"\s+", " ", s).strip()
    return s if s.endswith(".") else (s + ".")

def _softmax_two(logits: torch.Tensor):
    """ì´ì§„ ë¶„ë¥˜ ê°€ì •: p0, p1ê³¼ gap(|p1 - p0|) ë°˜í™˜."""
    if logits.size(-1) == 1:
        p1 = torch.sigmoid(logits)[0, 0]
        p0 = 1 - p1
    else:
        probs = torch.softmax(logits, dim=-1)[0]
        p0, p1 = probs[0], probs[1]
    gap = torch.abs(p1 - p0)
    return p0.item(), p1.item(), gap.item()

@torch.no_grad()
def model_forward(mdl, tok, text: str):
    enc = tok(text, return_tensors="pt", truncation=True, padding=True, max_length=256)
    for k in enc:
        enc[k] = enc[k].to(DEVICE)
    out = mdl(**enc)
    logits = out.logits
    p0, p1, gap = _softmax_two(logits)
    return p0, p1, gap  # p1 = "ì•…ì„±" í™•ë¥ 

# ===== 4) ë©”íƒ€ ê°€ì¤‘ì¹˜ íƒì§€ =====
ACTION_PAT = re.compile(
    r"(í•´ì¤˜|ë§Œë“¤ì–´|êµ¬í˜„(í•´|í•˜)|ì½”ë“œ(ë¥¼|ë¡œ)|ìŠ¤í¬ë¦½íŠ¸|ìë™í™”|ìš°íšŒ|ìš°íšŒí•´|ì‹¤í–‰í•´|ì‹¤í–‰ë²•|ê³µê²©í•´|ëš«(ì–´|ëŠ”)|ìš°íšŒí•˜ëŠ”|ì°¨ë‹¨ í•´ì œ|í¬ë™|íŒ¨ì¹˜|ìµìŠ¤í”Œë¡œì‡|exploit|payload|ì‹¤í–‰ ê°€ëŠ¥í•œ|download|ë‹¤ìš´ë¡œë“œ|ì„¤ì¹˜í•´|ì£¼ì…í•´|inject)",
    re.IGNORECASE,
)
INFO_PAT = re.compile(
    r"(ì˜ë¯¸|ì •ì˜|ì›ë¦¬|ì´ìœ |ê°œë…|ë™ì‘ ì›ë¦¬|ì™œ|ë¬´ì—‡|ì–´ë–»ê²Œ( ë˜ëŠ”| ì‘ë™)|ì„¤ëª…í•´|ì„¤ëª…|ì •ë¦¬í•´|ì•Œë ¤ì¤˜|ìš”ì•½|ê°€ì´ë“œ|ê°€ë¥´ì³)",
    re.IGNORECASE,
)
DANGER_WORDS = [
    "í­íƒ„", "ddos", "ë””ë„ìŠ¤", "ì‚´í•´", "ì‚´ì¸", "ë¬´ê¸°", "ì´ê¸°", "í­ë°œë¬¼",
    "ëœì„¬ì›¨ì–´", "ë°±ë„ì–´", "malware", "ì•…ì„±ì½”ë“œ", "í‚¤ë¡œê±°", "ìµìŠ¤í”Œë¡œì‡", "exploit",
    "í•´í‚¹", "í¬ë™", "í¬ë˜í‚¹", "í”¼ì‹±", "ìŠ¤ë¯¸ì‹±",
    "ì‹ ìš©ì¹´ë“œ ë²ˆí˜¸", "ì¹´ë“œ ë²ˆí˜¸", "cvv", "ë¹„ë°€ë²ˆí˜¸ íƒˆì·¨", "ì€í–‰ ì •ë³´", "ê°œì¸ì •ë³´ íƒˆì·¨",
]

def detect_meta_flags(text: str):
    t = text.lower()
    is_action = bool(ACTION_PAT.search(t))
    is_info   = bool(INFO_PAT.search(t))
    is_danger = any(w.lower() in t for w in DANGER_WORDS)
    return is_action, is_info, is_danger

def apply_meta_weights(p1: float, gap: float, is_action: bool, is_info: bool, is_danger: bool):
    """ì›ì ìˆ˜ p1ì— ë©”íƒ€ ê°€ì¤‘ì¹˜ í•©ì‚°, ë¶ˆí™•ì‹¤(gap<GAP_THR)ì´ë©´ ê°ì‡ . ê²°ê³¼ëŠ” [0,1]ë¡œ í´ë¨í”„."""
    score = p1
    if is_action: score += W_ACTION
    if is_info:   score += W_INFO
    if is_danger: score += W_DOMAIN
    if gap < GAP_THR: score += W_UNCERT
    score = max(0.0, min(1.0, score))
    return score

# ===== 5) ì˜ˆì¸¡ íŒŒì´í”„ë¼ì¸ =====
def predict(text: str, thr_ui: float):
    text = preprocess(text)
    mdl, tok, thr_ckpt, torch_loaded, tok_info = _cached_model()
    t0 = time.time()

    if torch_loaded:
        _, p1, gap = model_forward(mdl, tok, text)    # p1 = ì•…ì„± í™•ë¥ (ì›ì ìˆ˜)
    else:
        p1, gap = 0.0, 0.0  # ëª¨ë¸ ë¯¸ë¡œë”© ì‹¤íŒ¨ ì‹œ ì•ˆì „ ìª½ìœ¼ë¡œ

    is_action, is_info, is_danger = detect_meta_flags(text)
    adj_score = apply_meta_weights(p1, gap, is_action, is_info, is_danger)

    thr = float(thr_ui if thr_ui is not None else thr_ckpt)
    label = "ì•…ì„±" if adj_score >= thr else "ì•ˆì „"

    return {
        "ì›ì ìˆ˜(p1)": round(p1, 3),
        "ì¡°ì •ì ìˆ˜(p1+ê°€ì¤‘ì¹˜)": round(adj_score, 3),
        "ì„ê³„ê°’": round(thr, 3),
        "íŒì •": label,
        "ê·¼ê±°": {
            "softmax_gap(|p1-p0|)": round(gap, 3),
            "í”Œë˜ê·¸": {
                "ì‹¤í–‰í˜•_action": is_action,
                "ì„¤ëª…í˜•_info": is_info,
                "ìœ„í—˜ë‹¨ì–´_domain": is_danger,
            },
        },
        "_elapsed_s": round(time.time() - t0, 2),
        "ì„¸ë¶€": {"device": str(DEVICE), "tokenizer": tok_info, "torch_loaded": bool(torch_loaded)},
    }

# ===== 6) UI =====
st.title("ğŸ›¡ï¸ KillSwitch AI")

# â”€â”€ ì‚¬ì´ë“œë°” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if "OPENAI_API_KEY" not in st.session_state:
    st.session_state.OPENAI_API_KEY = ""

st.sidebar.header("ì„¤ì •")
OPENAI_API_KEY = st.sidebar.text_input("OPENAI_API_KEY", value=st.session_state.OPENAI_API_KEY, type="password")
if OPENAI_API_KEY != st.session_state.OPENAI_API_KEY:
    st.session_state.OPENAI_API_KEY = OPENAI_API_KEY

openai_model = st.sidebar.text_input("OpenAI ëª¨ë¸", value="gpt-4o-mini")
thr_ui       = st.sidebar.slider("ì„ê³„ê°’(ì°¨ë‹¨ ê¸°ì¤€)", 0.05, 0.95, 0.70, step=0.05)
force_call   = st.sidebar.checkbox("ìœ„í—˜í•´ë„ GPT í˜¸ì¶œ ê°•í–‰", value=False)
expert_mode  = st.sidebar.toggle("ğŸ› ï¸ ì „ë¬¸ê°€ ëª¨ë“œ (ì„¸ë¶€ ì§€í‘œ/JSONë§Œ)", value=False)

with st.sidebar.expander("HF ì—°ê²° ì ê²€"):
    st.caption(f"HF: {REPO_ID} ({REPO_TYPE}) / {FILENAME}")
    if st.button("ì²´í¬"):
        try:
            p = get_ckpt_path()
            size_mb = os.path.getsize(p) / 1_048_576
            st.success(f"OK: {os.path.basename(p)} Â· {size_mb:.1f} MB")
        except Exception as e:
            st.error("ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨")
            st.exception(e)

# â”€â”€ ë©”ì¸ ì…ë ¥ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
txt = st.text_area("í”„ë¡¬í”„íŠ¸", height=140, placeholder="ì˜ˆ) ì¸ì²œ ë§›ì§‘ ì•Œë ¤ì¤˜")

def summarize_reason(result: dict) -> str:
    flags = result["ê·¼ê±°"]["í”Œë˜ê·¸"]
    reasons = []
    if flags["ìœ„í—˜ë‹¨ì–´_domain"]: reasons.append("ìœ„í—˜ í‚¤ì›Œë“œ í¬í•¨")
    if flags["ì‹¤í–‰í˜•_action"]:   reasons.append("ì‹¤í–‰ ì§€ì‹œì–´ íƒì§€")
    if flags["ì„¤ëª…í˜•_info"]:     reasons.append("ì •ë³´ ìš”ì²­ ìœ„ì£¼")
    g = result["ê·¼ê±°"]["softmax_gap(|p1-p0|)"]
    conf = "ë†’ìŒ" if g >= 0.40 else ("ë³´í†µ" if g >= 0.20 else "ë‚®ìŒ")
    msg = " Â· ".join(reasons) if reasons else "ì •ìƒì ì¸ ì•ˆë‚´ ìš”ì²­"
    return f"{msg} Â· í™•ì‹ ë„: {conf}"

@st.cache_resource(show_spinner=False)
def get_openai_client():
    try:
        from openai import OpenAI
    except Exception as e:
        raise RuntimeError(f"openai ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸° ì‹¤íŒ¨: {e}")
    api_key = (
        st.secrets.get("OPENAI_API_KEY")
        or os.getenv("OPENAI_API_KEY")
        or st.session_state.OPENAI_API_KEY
    )
    return OpenAI(api_key=api_key) if api_key else None

# â”€â”€ ë²„íŠ¼ & ì¶œë ¥ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if st.button("ë¶„ì„"):
    if not (txt and txt.strip()):
        st.warning("í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
    else:
        with st.spinner("ë¶„ì„ ì¤‘..."):
            result = predict(txt, thr_ui)
        st.success(f"ë¶„ì„ ì™„ë£Œ ({result['_elapsed_s']:.2f}s)")

        verdict = result["íŒì •"]
        reason = summarize_reason(result)

        # â–¶ ì „ì‹œìš© íŒì • ì¹´ë“œ
        if verdict == "ì•…ì„±":
            st.markdown(
                f"""
                <div style="padding:18px;border-radius:14px;background:#ffe8e6;border:1px solid #ffb4ac">
                  <div style="font-size:22px;font-weight:700;color:#8c1d18;">âš ï¸ ìœ„í—˜í•œ ìš”ì²­ìœ¼ë¡œ íŒë‹¨í–ˆìŠµë‹ˆë‹¤</div>
                  <div style="margin-top:6px;font-size:16px;color:#5a1a17;">ì‚¬ìœ : {reason}</div>
                </div>
                """,
                unsafe_allow_html=True,
            )
        else:
            st.markdown(
                f"""
                <div style="padding:18px;border-radius:14px;background:#e8f5e9;border:1px solid #b7e1be">
                  <div style="font-size:22px;font-weight:700;color:#1b5e20;">âœ… ì•ˆì „í•œ ìš”ì²­ì…ë‹ˆë‹¤</div>
                  <div style="margin-top:6px;font-size:16px;color:#1b5e20;">ì‚¬ìœ : {reason}</div>
                </div>
                """,
                unsafe_allow_html=True,
            )

        # â–¶ GPT ì‘ë‹µ: ì „ë¬¸ê°€ ëª¨ë“œì™€ ë¬´ê´€í•˜ê²Œ í•­ìƒ ì„¹ì…˜ ë…¸ì¶œ
        st.markdown("### ğŸ¤– GPT ì‘ë‹µ")
        key_from_secrets = bool(st.secrets.get("OPENAI_API_KEY"))
        key_from_env     = bool(os.getenv("OPENAI_API_KEY"))
        key_from_session = bool(st.session_state.get("OPENAI_API_KEY"))
        key_ok = key_from_secrets or key_from_env or key_from_session

        allow_call = key_ok and (verdict == "ì•ˆì „" or (verdict == "ì•…ì„±" and force_call))

        if not key_ok:
            st.info("OPENAI_API_KEYê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•Šì•„ GPT í˜¸ì¶œì„ ìƒëµí•©ë‹ˆë‹¤.")
        elif not allow_call:
            st.warning("ì•…ì„± íŒì •ìœ¼ë¡œ GPT í˜¸ì¶œì´ ì°¨ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤. (ì‚¬ì´ë“œë°” 'ìœ„í—˜í•´ë„ GPT í˜¸ì¶œ ê°•í–‰'ì„ ì¼œë©´ í˜¸ì¶œë©ë‹ˆë‹¤)")
        else:
            try:
                client = get_openai_client()
                if client is None:
                    st.info("API Keyê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                else:
                    rsp = client.responses.create(
                        model=openai_model,
                        input=[
                            {"role": "system",
                             "content": "You are a helpful assistant. If the user's prompt is harmful or illegal, politely refuse and guide them safely."},
                            {"role": "user", "content": txt},
                        ],
                        temperature=0.3,
                        top_p=1.0,
                    )
                    st.write(rsp.output_text)
            except Exception as e:
                st.error(f"GPT í˜¸ì¶œ ì˜¤ë¥˜: {type(e).__name__}: {e}")

        # â–¶ ì „ë¬¸ê°€ ëª¨ë“œ: ì„¸ë¶€ ì§€í‘œ/JSONë§Œ
        if expert_mode:
            with st.expander("ğŸ” ì„¸ë¶€ ì§€í‘œ", expanded=False):
                c1, c2, c3, c4, c5 = st.columns([1.2, 1, 1, 1, 1])
                with c1: st.metric("íŒì •", verdict)
                with c2: st.metric("ì›ì ìˆ˜(p1)", f"{result['ì›ì ìˆ˜(p1)']:.3f}")
                with c3: st.metric("ì¡°ì •ì ìˆ˜", f"{result['ì¡°ì •ì ìˆ˜(p1+ê°€ì¤‘ì¹˜)']:.3f}")
                with c4: st.metric("ì„ê³„ê°’", f"{result['ì„ê³„ê°’']:.2f}")
                with c5: st.metric("í™•ì‹ ë„(gap)", f"{result['ê·¼ê±°']['softmax_gap(|p1-p0|)']:.3f}")
                st.caption(f"device={result['ì„¸ë¶€']['device']} Â· tokenizer={result['ì„¸ë¶€']['tokenizer']} Â· torch_loaded={result['ì„¸ë¶€']['torch_loaded']}")

            with st.expander("ğŸ§¾ ì›ë³¸ ê²°ê³¼(JSON)", expanded=False):
                st.json({k: v for k, v in result.items() if not k.startswith("_")})
