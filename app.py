# app.py â€” KillSwitch AI Â· Streamlit ë°ëª¨ (ìµœì¢…ë³¸)
# ----------------------------------------------------------------------
# âœ” HF í—ˆë¸Œì˜ ì²´í¬í¬ì¸íŠ¸(.pt) ìë™ ë‹¤ìš´ë¡œë“œ â†’ state_dict ë¡œë“œ
# âœ” ê·œì¹™(rule) + ëª¨ë¸ ì ìˆ˜ ìœµí•© (ë³´ìˆ˜ì : max)
# âœ” ì‚¬ì´ë“œë°”: ì„ê³„ê°’, ì…ë ¥ì–¸ì–´, ê°•í–‰ í˜¸ì¶œ, OpenAI í‚¤
# âœ” HF ì—°ê²° ì ê²€ ë²„íŠ¼ / torch_loaded í‘œì‹œ
# âœ” HF_DIR ì œê³µ ì‹œ from_pretrained ë””ë ‰í† ë¦¬ ì§ì ‘ ë¡œë“œ(ì™„ì „í•œ ëª¨ë¸ í˜•ì‹)
# ----------------------------------------------------------------------

import os, time, re, json
import streamlit as st

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 0) í˜ì´ì§€ ì„¤ì • (ìµœìƒë‹¨ì—ì„œ í•œ ë²ˆë§Œ)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(page_title="KillSwitch AI â€” Streamlit ë°ëª¨", layout="wide")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1) í™˜ê²½/ì‹œí¬ë¦¿ ì„¤ì •
#    - ì•„ë˜ ê¸°ë³¸ê°’ì€ Secrets/Envë¡œ ë®ì–´ì“¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
#      (Streamlit Cloud â†’ Settings â†’ Secrets)
#      HF_REPO_ID, HF_REPO_TYPE, HF_FILENAME, HF_TOKEN, HF_DIR
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DEFAULT_REPO_ID   = "your-username/killswitch-ai-checkpoints"   # ì˜ˆ: "beomsu/killswitch-ckpt"
DEFAULT_REPO_TYPE = "model"                                      # "model" ë˜ëŠ” "dataset"
DEFAULT_FILENAME  = "pt/prompt_guard_best.pt"                    # í—ˆë¸Œ ë‚´ë¶€ ê²½ë¡œ

REPO_ID   = st.secrets.get("HF_REPO_ID")   or os.getenv("HF_REPO_ID")   or DEFAULT_REPO_ID
REPO_TYPE = st.secrets.get("HF_REPO_TYPE") or os.getenv("HF_REPO_TYPE") or DEFAULT_REPO_TYPE
FILENAME  = st.secrets.get("HF_FILENAME")  or os.getenv("HF_FILENAME")  or DEFAULT_FILENAME
HF_TOKEN  = st.secrets.get("HF_TOKEN")     or os.getenv("HF_TOKEN")     # ë¹„ê³µê°œ ë¦¬í¬ë©´ í•„ìˆ˜
HF_DIR    = st.secrets.get("HF_DIR")       or os.getenv("HF_DIR")       # ì™„ì „ ëª¨ë¸ ë””ë ‰í† ë¦¬

# ëª¨ë¸ ë² ì´ìŠ¤/ë¼ë²¨ ìˆ˜ (í•™ìŠµ ì„¤ì •ì— ë§ê²Œ)
BASE_MODEL = os.getenv("BASE_MODEL") or "microsoft/deberta-v3-base"
NUM_LABELS = int(os.getenv("NUM_LABELS") or 2)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2) Hugging Face ì²´í¬í¬ì¸íŠ¸ ë¡œë”
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import torch
from huggingface_hub import hf_hub_download
from transformers import AutoTokenizer, AutoModelForSequenceClassification

@st.cache_resource(show_spinner=False)
def get_ckpt_path() -> str:
    """í—ˆë¸Œì—ì„œ .pt í•œ íŒŒì¼ì„ ë°›ì•„ ë¡œì»¬ ìºì‹œì— ì €ì¥í•˜ê³  ê²½ë¡œ ë°˜í™˜.
       ë¨¼ì € ./model/prompt_guard_best.pt ê°€ ìˆìœ¼ë©´ ê·¸ê±¸ ìš°ì„  ì‚¬ìš©."""
    # 0) ë¡œì»¬ ìš°ì„  (ì•± ë ˆí¬ì— í¬í•¨í–ˆì„ ë•Œ)
    local = os.path.join("model", "prompt_guard_best.pt")
    if os.path.exists(local):
        return local

    # 1) ì„ ì–¸ëœ repo_typeìœ¼ë¡œ ì‹œë„
    try:
        return hf_hub_download(
            repo_id=REPO_ID,
            filename=FILENAME,
            repo_type=REPO_TYPE,
            token=HF_TOKEN
        )
    except Exception as e1:
        # 2) ë°˜ëŒ€ íƒ€ì…ë„ í•œ ë²ˆ ì‹œë„ (datasetì— ì˜¬ë ¤ë‘” ê²½ìš°ë¥¼ ëŒ€ë¹„)
        alt = "dataset" if REPO_TYPE == "model" else "model"
        try:
            p = hf_hub_download(
                repo_id=REPO_ID,
                filename=FILENAME,
                repo_type=alt,
                token=HF_TOKEN
            )
            st.info(f"repo_type='{REPO_TYPE}' ì‹¤íŒ¨ â†’ '{alt}'ë¡œ ì„±ê³µ")
            return p
        except Exception as e2:
            st.error("í•™ìŠµ ì²´í¬í¬ì¸íŠ¸ë¥¼ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤(ê·œì¹™ ê¸°ë°˜ë§Œ ì‚¬ìš©). ìƒì„¸:")
            st.caption(str(e1))
            st.caption(str(e2))
            raise

@st.cache_resource(show_spinner=False)
def load_model_tokenizer():
    """í† í¬ë‚˜ì´ì €/ëª¨ë¸ ë¡œë“œ.
       1) HF_DIR ì§€ì • ì‹œ í•´ë‹¹ ë””ë ‰í† ë¦¬ì—ì„œ from_pretrained
       2) ì•„ë‹ˆë©´ BASE_MODEL ë¡œë“œ í›„ .pt state_dict ë®ì–´ì“°ê¸°
    """
    # í† í¬ë‚˜ì´ì €: sentencepiece í•„ìš” â†’ requirements.txtì— í¬í•¨
    try:
        tok = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=False)
    except Exception as e:
        # í™˜ê²½ì— sentencepiece ì—†ì„ ë•Œ fastë¡œ ì¬ì‹œë„
        st.info("ìŠ¬ë¡œìš° í† í¬ë‚˜ì´ì € ë¡œë“œ ì‹¤íŒ¨ â†’ fast í† í¬ë‚˜ì´ì €ë¡œ ì¬ì‹œë„í•©ë‹ˆë‹¤.")
        tok = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)

    # 1) ì™„ì „ ëª¨ë¸ ë””ë ‰í† ë¦¬
    if HF_DIR and os.path.isdir(HF_DIR):
        mdl = AutoModelForSequenceClassification.from_pretrained(HF_DIR)
        mdl.eval()
        return mdl, tok, 0.5, True  # thr=0.5, torch_loaded=True (ì™„ì „ ëª¨ë¸ í˜•ì‹)

    # 2) ë² ì´ìŠ¤ ëª¨ë¸ + ì²´í¬í¬ì¸íŠ¸ state_dict
    mdl = AutoModelForSequenceClassification.from_pretrained(BASE_MODEL, num_labels=NUM_LABELS)
    thr = 0.5
    torch_loaded = False
    try:
        ckpt_path = get_ckpt_path()
        ckpt = torch.load(ckpt_path, map_location="cpu", weights_only=False)
        state = ckpt["model"] if isinstance(ckpt, dict) and "model" in ckpt else ckpt
        missing, unexpected = mdl.load_state_dict(state, strict=False)
        if missing or unexpected:
            st.caption(f"state_dict mismatch â†’ missing:{len(missing)}, unexpected:{len(unexpected)}")
        thr = float(ckpt.get("val_thr", 0.5)) if isinstance(ckpt, dict) else 0.5
        torch_loaded = True
    except Exception as e:
        st.info("í•™ìŠµ ì²´í¬í¬ì¸íŠ¸ë¥¼ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤(ê·œì¹™ ê¸°ë°˜ë§Œ ì‚¬ìš©). ìƒì„¸:")
        st.caption(str(e))
    mdl.eval()
    return mdl, tok, thr, torch_loaded

@st.cache_resource(show_spinner=False)
def _cached_model():
    return load_model_tokenizer()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3) ê·œì¹™ ê¸°ë°˜ íƒì§€ (ê°„ë‹¨ í‚¤ì›Œë“œ)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
BAD_PATTERNS = [
    r"ìì‚´|ìí•´|ê·¹ë‹¨ì \s*ì„ íƒ",
    r"í­íƒ„|ì´ê¸°|ì‚´ì¸|í…ŒëŸ¬",
    r"ë§ˆì•½|í•„ë¡œí°|ì½”ì¹´ì¸",
    r"í•´í‚¹|ë””ë„ìŠ¤|ëœì„¬ì›¨ì–´|ì·¨ì•½ì \s*ì•…ìš©",
    r"ë¹„ë°€ë²ˆí˜¸|íŒ¨ìŠ¤ì›Œë“œ|OTP|ë°±ë„ì–´",
    r"ì£¼ë¯¼ë“±ë¡ë²ˆí˜¸|ì—¬ê¶Œë²ˆí˜¸|ì‹ ìš©ì¹´ë“œ\s*ë²ˆí˜¸",
    r"í”¼ì‹±|ë³´ì´ìŠ¤\s*í”¼ì‹±|ë©”ì‹ ì €\s*í”¼ì‹±|phishing",
    # ì‚´í•´/ì‚´ìƒ ì¸í…íŠ¸(í•œêµ­ì–´/ì˜ì–´)
    r"(ì‚¬ëŒ|íƒ€ì¸|ìƒëŒ€|ëˆ„êµ¬|ê·¸ë…€|ê·¸ë†ˆ|ê·¸ìƒˆë¼).{0,6}ì£½(ì—¬|ì¼|ì´|ì—¬ì¤˜|ì—¬ë¼|ì´ê²Œ|ì´ëŠ”|ì´ëŠ”ë²•)",
    r"ì£½ì—¬ì¤˜|ì£½ì´ëŠ”\s*ë°©ë²•|ì£½ì¼\s*ë°©ë²•|ì£½ì´ëŠ”ë²•|ì£½ì¼ë˜|ì£½ì—¬\s*ë²„ë ¤",
    r"kill( someone| him| her| them)?|how to kill|murder"
]
BAD_RE = re.compile("|".join(BAD_PATTERNS), re.I)


def rule_detect(text: str):
    if not text.strip():
        return 0.0, []
    found = sorted(set([m.group(0) for m in BAD_RE.finditer(text)]))
    score = min(1.0, len(found) * 0.4) if found else 0.0  # í‚¤ì›Œë“œë‹¹ 0.4 ê°€ì¤‘
    return score, found

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4) ì¶”ë¡  & ìœµí•© ë¡œì§
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def predict(text: str, thr_ui: float):
    mdl, tok, thr_ckpt, torch_loaded = _cached_model()
    t0 = time.time()

    # â‘  ê·œì¹™ ì ìˆ˜
    r_score, r_keys = rule_detect(text)

    # â‘¡ ëª¨ë¸ ì ìˆ˜
    m_score = 0.0
    if torch_loaded:
        enc = tok(text, return_tensors="pt", truncation=True, padding=True, max_length=256)
        with torch.no_grad():
            logits = mdl(**enc).logits
            if logits.size(-1) == 1:   # ì‹œê·¸ëª¨ì´ë“œ í—¤ë“œ
                m_score = torch.sigmoid(logits)[0, 0].item()
            else:                      # ì†Œí”„íŠ¸ë§¥ìŠ¤ í—¤ë“œ
                m_score = 1.0 - torch.softmax(logits, dim=-1)[0, 1].item()   # benign í™•ë¥  â†’ ì•…ì„± í™•ë¥ ë¡œ ë°˜ì „

    # â‘¢ ìœµí•©: ë³´ìˆ˜ì ìœ¼ë¡œ max ì‚¬ìš©
    score = max(r_score, m_score)
    thr = float(thr_ui if thr_ui is not None else thr_ckpt)
    label = "ì•…ì„±" if score >= thr else "ì•ˆì „"

    return {
        "ì ìˆ˜": round(score, 3),
        "ì„ê³„ê°’": round(thr, 3),
        "íŒì •": label,
        "í‚¤ì›Œë“œ": r_keys or ["-"],
        "ì„¸ë¶€": {
            "rule_score": round(r_score, 3),
            "model_score": round(m_score, 3),
            "torch_loaded": bool(torch_loaded),
        },
        "_elapsed_s": round(time.time() - t0, 2),
    }

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 5) UI
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 5) UI  â† ê¸°ì¡´ ë²„íŠ¼ 2ê°œ ì“°ë˜ ë¸”ë¡ì„ ì´ê±¸ë¡œ í†µì§¸ë¡œ êµì²´
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.title("ğŸ›¡ï¸ KillSwitch AI â€” Streamlit ë°ëª¨")

# ì‚¬ì´ë“œë°”
st.sidebar.header("ì„¤ì •")
OPENAI_API_KEY = st.sidebar.text_input("OPENAI_API_KEY", type="password")
openai_model   = st.sidebar.text_input("OpenAI ëª¨ë¸", value="gpt-4o-mini")
thr_ui         = st.sidebar.slider("ì„ê³„ê°’(ì°¨ë‹¨ ê¸°ì¤€)", 0.05, 0.95, 0.70, step=0.05)
input_lang     = st.sidebar.selectbox("ì…ë ¥ ì–¸ì–´", ["auto", "ko", "en"], index=0)
force_call     = st.sidebar.checkbox("ìœ„í—˜í•´ë„ GPT í˜¸ì¶œ ê°•í–‰", value=False)

# HF ì—°ê²° ì ê²€ (ê·¸ëŒ€ë¡œ ìœ ì§€)
st.sidebar.caption(f"HF: {REPO_ID} ({REPO_TYPE}) / {FILENAME}")
if st.sidebar.button("HF ì—°ê²° ì ê²€"):
    try:
        p = get_ckpt_path()
        size_mb = os.path.getsize(p) / 1_048_576
        st.sidebar.success(f"OK: {os.path.basename(p)} Â· {size_mb:.1f} MB")
    except Exception as e:
        st.sidebar.error("ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨")
        st.sidebar.exception(e)

# ì…ë ¥ & ë²„íŠ¼ (ë‹¨ì¼ ë²„íŠ¼)
txt = st.text_area("í”„ë¡¬í”„íŠ¸", height=140, placeholder="ì˜ˆ) ì¸ì²œ ë§›ì§‘ ì•Œë ¤ì¤˜")
run = st.button("ë¶„ì„ (GPT í˜¸ì¶œ)")

if run:
    with st.spinner("ë¶„ì„ ì¤‘..."):
        result = predict(txt, thr_ui=thr_ui)
    st.success(f"ë¶„ì„ ì™„ë£Œ ({result['_elapsed_s']:.2f}s)")

    # ë¶„ì„ ê²°ê³¼
    st.subheader("ë¶„ì„ ê²°ê³¼  â†ªï¸")
    st.json({k: v for k, v in result.items() if not k.startswith("_")})

    # GPT ì‘ë‹µ (í•œ ë²„íŠ¼ ì•ˆì—ì„œ í›„ì† ì‹¤í–‰)
    st.subheader("GPT ì‘ë‹µ")
    if not OPENAI_API_KEY:
        st.info("OPENAI_API_KEYê°€ ì—†ì–´ GPT í˜¸ì¶œì€ ìƒëµí–ˆìŠµë‹ˆë‹¤.")
    elif result["íŒì •"] == "ì•…ì„±" and not force_call:
        st.warning("ì•…ì„±ìœ¼ë¡œ íŒì •ë˜ì–´ GPT í˜¸ì¶œì„ ì°¨ë‹¨í–ˆìŠµë‹ˆë‹¤. (ì‚¬ì´ë“œë°” 'ê°•í–‰'ì„ ì²´í¬í•˜ë©´ í˜¸ì¶œ)")
    else:
        try:
            from openai import OpenAI
            client = OpenAI(api_key=OPENAI_API_KEY)
            sys_prompt = (
                "You are a helpful assistant. If the user's prompt is clearly harmful or illegal, "
                "politely refuse and provide safe alternatives."
            )
            msgs = [{"role": "system", "content": sys_prompt},
                    {"role": "user", "content": txt}]
            rsp = client.chat.completions.create(
                model=openai_model,
                messages=msgs,
                temperature=0.3,
                top_p=1.0,
            )
            st.write(rsp.choices[0].message.content.strip())
        except Exception as e:
            st.error(f"GPT í˜¸ì¶œ ì˜¤ë¥˜: {type(e).__name__}: {e}")
            st.caption("429(ì¿¼í„° ì´ˆê³¼) ë“± ìš”ê¸ˆì œ/ëª¨ë¸ ì´ë¦„ì„ í™•ì¸í•˜ì„¸ìš”.")
