# app.py â€” KillSwitch AI (ckpt í—¤ë“œ ì¬ì¥ì°© Â· ì •ìƒ ì¶”ë¡  ë²„ì „)
# ----------------------------------------------------------------------
# âœ” HF .pt ì²´í¬í¬ì¸íŠ¸ì—ì„œ classifier head(W,b) ì¶”ì¶œ â†’ ë°±ë³¸ ì„ë² ë”© + Linearë¡œ ì§ì ‘ ì¶”ë¡ 
# âœ” hp.model_nameì´ ìˆìœ¼ë©´ ê·¸ ë°±ë³¸(AutoModel) ìš°ì„  ì‚¬ìš©, ì‹¤íŒ¨ ì‹œ ì°¨ì› ë§ëŠ” í›„ë³´ ìë™ ì„ íƒ
# âœ” í’€ë§: Mean pooling (L2 ì •ê·œí™” ì—†ìŒ; í…ŒìŠ¤íŠ¸ì—ì„œ ê°€ì¥ ë‚«ê²Œ ë‚˜ì˜´)
# âœ” ì„ê³„ê°’ ê¸°ë³¸ê°’ 0.5 (ë°ëª¨/íŒë„¬ìš©), ì‚¬ì´ë“œë°”ë¡œ ì¡°ì ˆ
# âœ” RULE_PATH í”ŒëŸ¬ê·¸ì¸(.txt/.json/.yml)ì´ ì œê³µë  ë•Œë§Œ ê·œì¹™ ê¸°ë°˜ í•¨ê»˜ ì‚¬ìš©
# âœ” OpenAI ê°•í–‰ í˜¸ì¶œ ì˜µì…˜ ìœ ì§€
# ----------------------------------------------------------------------

import os, re, json, time
import streamlit as st
import numpy as np
import torch
import torch.nn as nn

from huggingface_hub import hf_hub_download
from transformers import AutoTokenizer, AutoModel

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Streamlit ê¸°ë³¸ ì„¤ì •
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(page_title="KillSwitch AI", layout="wide")
st.title("ğŸ›¡ï¸ KillSwitch AI")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# í™˜ê²½ì„¤ì •
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# HF ìœ„ì¹˜ (ê¸°ë³¸: ê³µê°œ ë¦¬í¬)
REPO_ID   = os.getenv("HF_REPO_ID", "cookiechips/KillSwitch_ai")
CKPT_NAME = os.getenv("CKPT_NAME", "prompt_guard_best.pt")
DEVICE    = "cpu"

# ë£° í”ŒëŸ¬ê·¸ì¸ (RULE_PATHê°€ ìˆì„ ë•Œë§Œ í™œì„±)
USE_RULE_ENV = os.getenv("USE_RULE", "on").lower() in ["on", "1", "true"]
RULE_PATH    = os.getenv("RULE_PATH") or (st.secrets.get("RULE_PATH") if "RULE_PATH" in st.secrets else None)

# ì„ê³„ê°’ ê¸°ë³¸ê°’ (ìš”ì²­ ë°˜ì˜: 0.5)
DEFAULT_SLIDER_THR = float(os.getenv("DEFAULT_THR") or 0.5)

# ì•ˆì „ ë¡œë“œ (PyTorch 2.6 weights_only ê¸°ë³¸ ëŒ€ì‘)
torch.serialization.add_safe_globals([dict, list, tuple, set, np.ndarray, np.generic])

# ì„ë² ë”© ì°¨ì›ë³„ ë°±ë³¸ í›„ë³´ (ë°±ì—… ê²½ë¡œ)
BACKBONE_CANDIDATES = [
    ("microsoft/deberta-v3-base", 768),
    ("sentence-transformers/all-mpnet-base-v2", 768),
    ("sentence-transformers/paraphrase-mpnet-base-v2", 768),
    ("sentence-transformers/paraphrase-xlm-r-multilingual-v1", 768),
    ("sentence-transformers/all-MiniLM-L6-v2", 384),
    ("sentence-transformers/paraphrase-MiniLM-L12-v2", 384),
]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ìœ í‹¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def mean_pooling(last_hidden_state, attention_mask):
    mask = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()
    summed = (last_hidden_state * mask).sum(dim=1)
    denom  = torch.clamp(mask.sum(dim=1), min=1e-9)
    return summed / denom

def extract_head(sd: dict):
    """state_dictì—ì„œ (W,b) ì¶”ì¶œ"""
    cands = [
        ("classifier.weight","classifier.bias"),
        ("classification_head.weight","classification_head.bias"),
        ("head.weight","head.bias"),
        ("fc.weight","fc.bias"),
        ("linear.weight","linear.bias"),
        ("dense.weight","dense.bias"),
        ("proj.weight","proj.bias"),
    ]
    for w,b in cands:
        if w in sd and b in sd:
            W, B = sd[w].clone().detach(), sd[b].clone().detach()
            return W, B, w, b
    # ëª¨ì–‘ìœ¼ë¡œ ì¶”ë¡ 
    for k,v in sd.items():
        if isinstance(v, torch.Tensor) and v.ndim==2:
            nl, hd = v.shape
            for kb, vb in sd.items():
                if kb.endswith("bias") and isinstance(vb, torch.Tensor) and vb.ndim==1 and vb.shape[0]==nl:
                    return v.clone().detach(), vb.clone().detach(), k, kb
    return None, None, None, None

def resolve_pos_idx(hp: dict|None):
    """ì•…ì„±(positive) í´ë˜ìŠ¤ ì¸ë±ìŠ¤ ì¶”ì • (ê¸°ë¡ ì—†ìœ¼ë©´ 1)"""
    if isinstance(hp, dict):
        for k in ["pos_index","positive_index","positive_id","pos_cls"]:
            if k in hp:
                try: return int(hp[k])
                except: pass
        lm = hp.get("label_map") or hp.get("id2label") or hp.get("labels")
        if isinstance(lm, dict):
            for a,b in lm.items():
                if any(x in str(a).lower() for x in ["malicious","toxic","positive","ì•…ì„±"]):
                    try: return int(b)
                    except: pass
                if any(x in str(b).lower() for x in ["malicious","toxic","positive","ì•…ì„±"]):
                    try: return int(a)
                    except: pass
    return 1

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë£° í”ŒëŸ¬ê·¸ì¸ ë¡œë”
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_resource(show_spinner=False)
def load_rule_re():
    if not (USE_RULE_ENV and RULE_PATH): return None
    try:
        ext = os.path.splitext(RULE_PATH)[1].lower()
        pats = []
        if ext == ".txt":
            with open(RULE_PATH, "r", encoding="utf-8") as f:
                for line in f:
                    s = line.strip()
                    if s and not s.startswith("#"): pats.append(s)
        elif ext == ".json":
            with open(RULE_PATH, "r", encoding="utf-8") as f:
                data = json.load(f)
                if isinstance(data, list): pats = [str(x) for x in data if str(x).strip()]
        elif ext in [".yaml",".yml"]:
            import yaml
            with open(RULE_PATH, "r", encoding="utf-8") as f:
                data = yaml.safe_load(f)
                if isinstance(data, list): pats = [str(x) for x in data if str(x).strip()]
        else:
            st.warning(f"RULE_PATH í™•ì¥ì ë¯¸ì§€ì›: {ext}")
            return None
        if not pats: return None
        return re.compile("|".join(pats), re.I)
    except Exception as e:
        st.info(f"ë£° ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None

RULE_RE = load_rule_re()
USE_RULE = RULE_RE is not None

def rule_detect(text: str):
    if not (USE_RULE and text.strip()): return 0.0
    return 1.0 if RULE_RE.search(text) else 0.0

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ëª¨ë¸/í† í¬ë‚˜ì´ì €/í—¤ë“œ ë¡œë”© (í•µì‹¬)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_resource(show_spinner=False)
def load_model_bundle():
    # 1) ckpt ë¡œë“œ
    ckpt_path = hf_hub_download(repo_id=REPO_ID, filename=CKPT_NAME, repo_type="model")
    ckpt = torch.load(ckpt_path, map_location="cpu", weights_only=False)
    if not isinstance(ckpt, dict):
        raise RuntimeError("ì²´í¬í¬ì¸íŠ¸ ìµœìƒìœ„ê°€ dictê°€ ì•„ë‹™ë‹ˆë‹¤.")
    state = ckpt.get("model", {})
    hp    = ckpt.get("hp", {}) or {}
    best_thr = float(ckpt.get("best_thr", 0.0) or 0.0)

    # 2) ë¶„ë¥˜ í—¤ë“œ ì¶”ì¶œ
    W, B, w_key, b_key = extract_head(state)
    if W is None:
        raise RuntimeError("ì²´í¬í¬ì¸íŠ¸ì—ì„œ ë¶„ë¥˜ í—¤ë“œ(W,b)ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
    num_labels, needed_dim = W.shape

    # 3) ë°±ë³¸ ì„ íƒ â€” hp.model_name ìš°ì„ , ì‹¤íŒ¨ ì‹œ ì°¨ì› ë§ëŠ” í›„ë³´
    tokenizer = None; backbone = None; picked = None
    tried = []

    prefer = []
    if isinstance(hp, dict) and hp.get("model_name"):
        prefer.append((hp["model_name"], needed_dim))
    prefer.extend([c for c in BACKBONE_CANDIDATES if c not in prefer])

    for model_id, emb_dim in prefer:
        if emb_dim != needed_dim: continue
        try:
            tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)
            backbone  = AutoModel.from_pretrained(model_id).to(DEVICE).eval()
            picked = model_id
            break
        except Exception as e:
            tried.append((model_id, str(e)))
            continue

    if backbone is None:
        raise RuntimeError(f"ì„ë² ë”© {needed_dim}ì°¨ì›ì— ë§ëŠ” ë°±ë³¸ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì‹œë„: {tried[:3]}")

    # 4) Linear head ì¬êµ¬ì„± + ì£¼ì…
    head = nn.Linear(needed_dim, num_labels, bias=True).to(DEVICE).eval()
    with torch.no_grad():
        head.weight.copy_(W)
        head.bias.copy_(B)

    # 5) pos idx ì¶”ì •
    pos_idx = resolve_pos_idx(hp)

    return {
        "tokenizer": tokenizer,
        "backbone": backbone,
        "head": head,
        "picked": picked,
        "pos_idx": pos_idx,
        "best_thr": best_thr,
        "needed_dim": needed_dim,
        "num_labels": num_labels,
        "ckpt_path": ckpt_path,
        "hp": hp,
    }

MDL = load_model_bundle()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì˜ˆì¸¡
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_data(show_spinner=False)
def predict_once(text: str, thr_ui: float|None):
    t0 = time.time()
    tok = MDL["tokenizer"]; bb = MDL["backbone"]; head = MDL["head"]; pos_idx = MDL["pos_idx"]
    # 1) ë£° ì ìˆ˜
    r_score = rule_detect(text)
    # 2) ëª¨ë¸ ì ìˆ˜
    with torch.no_grad():
        enc = tok(text, return_tensors="pt", truncation=True, padding=True, max_length=256).to(DEVICE)
        out = bb(**enc)
        if not hasattr(out, "last_hidden_state"):
            raise RuntimeError("ë°±ë³¸ ì¶œë ¥ì— last_hidden_stateê°€ ì—†ìŠµë‹ˆë‹¤.")
        emb = mean_pooling(out.last_hidden_state, enc["attention_mask"])  # chosen
        probs = torch.softmax(head(emb), dim=-1).cpu().numpy()[0]
        m_score = float(probs[pos_idx])
    # 3) ì„ê³„ê°’ ë° ìœµí•©
    used_thr = float(thr_ui if thr_ui is not None else (MDL["best_thr"] if MDL["best_thr"]>0 else 0.5))
    score = max(r_score, m_score) if USE_RULE else m_score
    label = "ì•…ì„±" if score >= used_thr else "ì•ˆì „"
    return {
        "íŒì •": label,
        "ì ìˆ˜": round(score, 3),
        "ì„ê³„ê°’": round(used_thr, 3),
        "ì„¸ë¶€": {
            "rule_on": USE_RULE,
            "rule_score": round(r_score, 3),
            "model_score": round(m_score, 3),
            "pos_index": int(MDL["pos_idx"]),
            "backbone": MDL["picked"],
            "needed_dim": int(MDL["needed_dim"]),
            "best_thr_ckpt": float(MDL["best_thr"] or 0.0),
        },
        "_elapsed_s": round(time.time()-t0, 2),
    }

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# UI
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.sidebar.header("ì„¤ì •")
st.sidebar.caption(f"HF: {REPO_ID} / {CKPT_NAME}")
st.sidebar.caption(f"ë°±ë³¸: {MDL['picked']} Â· dim: {MDL['needed_dim']} Â· pos_idx: {MDL['pos_idx']}")
OPENAI_API_KEY = st.sidebar.text_input("OPENAI_API_KEY", type="password")
openai_model   = st.sidebar.text_input("OpenAI ëª¨ë¸", value="gpt-4o-mini")
thr_ui         = st.sidebar.slider("ì„ê³„ê°’(ì°¨ë‹¨ ê¸°ì¤€)", 0.05, 0.99, DEFAULT_SLIDER_THR, step=0.01)
input_lang     = st.sidebar.selectbox("ì…ë ¥ ì–¸ì–´", ["auto","ko","en"], index=0)
force_call     = st.sidebar.checkbox("ìœ„í—˜í•´ë„ GPT í˜¸ì¶œ ê°•í–‰", value=False)

txt = st.text_area("í”„ë¡¬í”„íŠ¸", height=140, placeholder="ì˜ˆ) ë³´ì´ìŠ¤ í”¼ì‹± ë©”ì‹œì§€ ë§Œë“¤ì–´ì¤˜")
run = st.button("ë¶„ì„ (GPT í˜¸ì¶œ)")

if run:
    result = predict_once(txt, thr_ui)
    st.success(f"ë¶„ì„ ì™„ë£Œ ({result['_elapsed_s']}s)")
    st.subheader("ë¶„ì„ ê²°ê³¼  â†ªï¸")
    st.json({k:v for k,v in result.items() if not k.startswith("_")})

    st.subheader("GPT ì‘ë‹µ")
    if not OPENAI_API_KEY:
        st.info("OPENAI_API_KEYê°€ ì—†ì–´ GPT í˜¸ì¶œì€ ìƒëµí–ˆìŠµë‹ˆë‹¤.")
    elif result["íŒì •"] == "ì•…ì„±" and not force_call:
        st.warning("ì•…ì„±ìœ¼ë¡œ íŒì •ë˜ì–´ GPT í˜¸ì¶œì„ ì°¨ë‹¨í–ˆìŠµë‹ˆë‹¤. (ì‚¬ì´ë“œë°” 'ê°•í–‰' ì²´í¬ ì‹œ í˜¸ì¶œ)")
    else:
        try:
            from openai import OpenAI
            client = OpenAI(api_key=OPENAI_API_KEY)
            sys_prompt = (
                "You are a helpful assistant. If the user's prompt is clearly harmful or illegal, "
                "politely refuse and provide safe alternatives."
            )
            msgs = [{"role":"system","content":sys_prompt},
                    {"role":"user","content":txt}]
            rsp = client.chat.completions.create(
                model=openai_model,
                messages=msgs,
                temperature=0.3,
                top_p=1.0,
            )
            st.write(rsp.choices[0].message.content.strip())
        except Exception as e:
            st.error(f"GPT í˜¸ì¶œ ì˜¤ë¥˜: {type(e).__name__}: {e}")
            st.caption("429(ì¿¼í„° ì´ˆê³¼) ë“± ëª¨ë¸ëª…/ìš”ê¸ˆì œë¥¼ í™•ì¸í•˜ì„¸ìš”.")
