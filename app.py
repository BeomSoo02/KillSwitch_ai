# app.py â€” KillSwitch AI Â· Streamlit ë°ëª¨ (ëª¨ë¸ ì „ìš© + ì™¸ë¶€ ë£° í”ŒëŸ¬ê·¸ì¸)
# ----------------------------------------------------------------------
# âœ” HF í—ˆë¸Œ ì²´í¬í¬ì¸íŠ¸(.pt) ìë™ ë‹¤ìš´ë¡œë“œ â†’ state_dict ë¡œë“œ
# âœ” ê¸°ë³¸: ëª¨ë¸ë§Œ ì‚¬ìš©. ë£°ì€ ì™¸ë¶€ íŒŒì¼(RULE_PATH) ìˆì„ ë•Œë§Œ í”ŒëŸ¬ê·¸ì¸ì²˜ëŸ¼ ë¡œë“œ
# âœ” USE_RULE(ê¸°ë³¸ on) + RULE_PATH ë‘˜ ë‹¤ ì¶©ì¡± ì‹œì—ë§Œ ë£° í™œì„±
# âœ” ìœµí•©: ë£° í™œì„± ì‹œ max(rule, model), ë¹„í™œì„± ì‹œ modelë§Œ
# âœ” ì‚¬ì´ë“œë°”: ì„ê³„ê°’, ì…ë ¥ì–¸ì–´, ê°•í–‰ í˜¸ì¶œ, OpenAI í‚¤, HF ì—°ê²° ì ê²€
# ----------------------------------------------------------------------

import os, time, re, json
import streamlit as st

st.set_page_config(page_title="KillSwitch AI", layout="wide")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1) í™˜ê²½/ì‹œí¬ë¦¿
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DEFAULT_REPO_ID   = "your-username/killswitch-ai-checkpoints"
DEFAULT_REPO_TYPE = "model"
DEFAULT_FILENAME  = "pt/prompt_guard_best.pt"

REPO_ID   = st.secrets.get("HF_REPO_ID")   or os.getenv("HF_REPO_ID")   or DEFAULT_REPO_ID
REPO_TYPE = st.secrets.get("HF_REPO_TYPE") or os.getenv("HF_REPO_TYPE") or DEFAULT_REPO_TYPE
FILENAME  = st.secrets.get("HF_FILENAME")  or os.getenv("HF_FILENAME")  or DEFAULT_FILENAME
HF_TOKEN  = st.secrets.get("HF_TOKEN")     or os.getenv("HF_TOKEN")
HF_DIR    = st.secrets.get("HF_DIR")       or os.getenv("HF_DIR")

BASE_MODEL = os.getenv("BASE_MODEL") or "microsoft/deberta-v3-base"
NUM_LABELS = int(os.getenv("NUM_LABELS") or 2)

# ë£° ê´€ë ¨(ê¸°ë³¸ onì´ì§€ë§Œ, ì™¸ë¶€ íŒŒì¼ ì—†ìœ¼ë©´ ìë™ ë¹„í™œì„±)
USE_RULE_ENV = os.getenv("USE_RULE", "on").lower() in ["on", "1", "true"]
RULE_PATH    = os.getenv("RULE_PATH") or st.secrets.get("RULE_PATH")  # ì˜ˆ: rules/patterns.txt

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2) ëª¨ë¸ ë¡œë”
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import torch
from huggingface_hub import hf_hub_download
from transformers import AutoTokenizer, AutoModelForSequenceClassification

@st.cache_resource(show_spinner=False)
def get_ckpt_path() -> str:
    local = os.path.join("model", "prompt_guard_best.pt")
    if os.path.exists(local):
        return local
    try:
        return hf_hub_download(repo_id=REPO_ID, filename=FILENAME, repo_type=REPO_TYPE, token=HF_TOKEN)
    except Exception as e1:
        alt = "dataset" if REPO_TYPE == "model" else "model"
        try:
            p = hf_hub_download(repo_id=REPO_ID, filename=FILENAME, repo_type=alt, token=HF_TOKEN)
            st.info(f"repo_type='{REPO_TYPE}' ì‹¤íŒ¨ â†’ '{alt}'ë¡œ ì„±ê³µ")
            return p
        except Exception as e2:
            st.error("í•™ìŠµ ì²´í¬í¬ì¸íŠ¸ë¥¼ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. (ë£° ë¹„í™œì„± ì‹œ ëª¨ë¸ë§Œ, ë£°ë„ ì—†ìœ¼ë©´ íŒì • ì œí•œ)")
            st.caption(str(e1)); st.caption(str(e2))
            raise

@st.cache_resource(show_spinner=False)
def load_model_tokenizer():
    try:
        tok = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=False)
    except Exception:
        st.info("ìŠ¬ë¡œìš° í† í¬ë‚˜ì´ì € ë¡œë“œ ì‹¤íŒ¨ â†’ fast í† í¬ë‚˜ì´ì €ë¡œ ì¬ì‹œë„í•©ë‹ˆë‹¤.")
        tok = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)

    if HF_DIR and os.path.isdir(HF_DIR):
        mdl = AutoModelForSequenceClassification.from_pretrained(HF_DIR)
        mdl.eval()
        return mdl, tok, 0.5, True

    mdl = AutoModelForSequenceClassification.from_pretrained(BASE_MODEL, num_labels=NUM_LABELS)
    thr = 0.5; torch_loaded = False
    try:
        ckpt_path = get_ckpt_path()
        ckpt = torch.load(ckpt_path, map_location="cpu", weights_only=False)
        state = ckpt["model"] if isinstance(ckpt, dict) and "model" in ckpt else ckpt
        missing, unexpected = mdl.load_state_dict(state, strict=False)
        if missing or unexpected:
            st.caption(f"state_dict mismatch â†’ missing:{len(missing)}, unexpected:{len(unexpected)}")
        thr = float(ckpt.get("val_thr", 0.5)) if isinstance(ckpt, dict) else 0.5
        torch_loaded = True
    except Exception as e:
        st.info("ì²´í¬í¬ì¸íŠ¸ë¥¼ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. (ë£° ì—†ìœ¼ë©´ íŒì •ì´ ì œí•œë  ìˆ˜ ìˆìŒ)")
        st.caption(str(e))
    mdl.eval()
    return mdl, tok, thr, torch_loaded

@st.cache_resource(show_spinner=False)
def _cached_model():
    return load_model_tokenizer()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3) ì™¸ë¶€ ë£° í”ŒëŸ¬ê·¸ì¸ ë¡œë”© (ì½”ë“œ ë‚´ í‚¤ì›Œë“œ ì—†ìŒ)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_resource(show_spinner=False)
def _load_external_rule():
    """
    RULE_PATHê°€ ì£¼ì–´ì§€ê³  USE_RULE_ENV=Trueì¼ ë•Œë§Œ ì™¸ë¶€ íŒŒì¼ì„ ë¡œë“œí•˜ì—¬ ì»´íŒŒì¼.
    ì§€ì› í¬ë§·:
      - .txt  : ì¤„ë‹¹ í•˜ë‚˜ì˜ ì •ê·œì‹
      - .json : ["pat1", "pat2", ...]
      - .yaml/.yml : ["pat1", "pat2", ...]
    ì‹¤íŒ¨ ì‹œ None ë°˜í™˜ â†’ ë£° ë¹„í™œì„±
    """
    if not USE_RULE_ENV or not RULE_PATH:
        return None

    try:
        ext = os.path.splitext(RULE_PATH)[1].lower()
        patterns = []
        if ext == ".txt":
            with open(RULE_PATH, "r", encoding="utf-8") as f:
                for line in f:
                    s = line.strip()
                    if s and not s.startswith("#"):
                        patterns.append(s)
        elif ext == ".json":
            with open(RULE_PATH, "r", encoding="utf-8") as f:
                data = json.load(f)
                if isinstance(data, list):
                    patterns = [str(x) for x in data if str(x).strip()]
        elif ext in [".yaml", ".yml"]:
            import yaml  # pyyaml í•„ìš”
            with open(RULE_PATH, "r", encoding="utf-8") as f:
                data = yaml.safe_load(f)
                if isinstance(data, list):
                    patterns = [str(x) for x in data if str(x).strip()]
        else:
            st.warning(f"RULE_PATH í™•ì¥ì ë¯¸ì§€ì›: {ext} (txt/json/yaml ê¶Œì¥)")
            return None

        if not patterns:
            st.info("RULE_PATHì—ì„œ ìœ íš¨í•œ íŒ¨í„´ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë£° ë¹„í™œì„±.")
            return None

        return re.compile("|".join(patterns), re.I)
    except Exception as e:
        st.info(f"ì™¸ë¶€ ë£° ë¡œë“œ ì‹¤íŒ¨ â†’ ë¹„í™œì„±í™”í•©ë‹ˆë‹¤. ìƒì„¸: {e}")
        return None

RULE_RE = _load_external_rule()
USE_RULE = bool(RULE_RE)  # ì‹¤ì œ í™œì„± ì—¬ë¶€(íŒŒì¼ ë¡œë”© ì„±ê³µ ì‹œì—ë§Œ True)

def rule_detect(text: str):
    if not USE_RULE or RULE_RE is None or not text.strip():
        return 0.0, None  # í‚¤ì›Œë“œ ëª©ë¡ì€ ë°˜í™˜í•˜ì§€ ì•ŠìŒ(ë¦¬í„°ëŸ´ ë…¸ì¶œ ë°©ì§€)
    found = list(RULE_RE.finditer(text))
    score = 1.0 if found else 0.0   # ì™¸ë¶€ ë£°ì€ ì¼ë‹¨ 0/1 ë°©ì‹(í•„ìš” ì‹œ ê°€ì¤‘ ë³€ê²½)
    return score, None

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4) ì¶”ë¡  & ìœµí•©
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def predict(text: str, thr_ui: float):
    mdl, tok, thr_ckpt, torch_loaded = _cached_model()
    t0 = time.time()

    # (1) ê·œì¹™ ì ìˆ˜
    r_score, _ = rule_detect(text)  # í‚¤ì›Œë“œ ëª©ë¡ ë¯¸ë…¸ì¶œ

    # (2) ëª¨ë¸ ì ìˆ˜
    m_score = 0.0
    if torch_loaded:
        enc = tok(text, return_tensors="pt", truncation=True, padding=True, max_length=256)
        with torch.no_grad():
            logits = mdl(**enc).logits
            m_score = torch.sigmoid(logits)[0, 0].item() if logits.size(-1) == 1 \
                      else torch.softmax(logits, dim=-1)[0, 1].item()
    elif not USE_RULE:
        st.error("ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ê³  ì™¸ë¶€ ë£°ë„ ë¹„í™œì„±í™”ë˜ì–´ íŒì •ì´ ë¶ˆê°€í•©ë‹ˆë‹¤.")
        return {"íŒì •": "ë¶ˆê°€", "ì ìˆ˜": 0.0, "ì„¸ë¶€": {"rule_on": USE_RULE, "torch_loaded": torch_loaded}}

    # (3) ìœµí•©
    score = max(r_score, m_score) if USE_RULE else m_score
    thr = float(thr_ui if thr_ui is not None else thr_ckpt)
    label = "ì•…ì„±" if score >= thr else "ì•ˆì „"

    return {
        "ì ìˆ˜": round(score, 3),
        "ì„ê³„ê°’": round(thr, 3),
        "íŒì •": label,
        "ì„¸ë¶€": {
            "rule_on": USE_RULE,
            "rule_src": (RULE_PATH if USE_RULE else None),
            "rule_score": round(r_score, 3),
            "model_score": round(m_score, 3),
            "torch_loaded": bool(torch_loaded),
        },
        "_elapsed_s": round(time.time() - t0, 2),
    }

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 5) UI
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.title("ğŸ›¡ï¸ KillSwitch AI")

st.sidebar.header("ì„¤ì •")
st.sidebar.caption(f"ê·œì¹™ ê¸°ë°˜ íƒì§€: {'ON' if USE_RULE else 'OFF'}"
                   + (f" Â· {RULE_PATH}" if USE_RULE else ""))
OPENAI_API_KEY = st.sidebar.text_input("OPENAI_API_KEY", type="password")
openai_model   = st.sidebar.text_input("OpenAI ëª¨ë¸", value="gpt-4o-mini")
thr_ui         = st.sidebar.slider("ì„ê³„ê°’(ì°¨ë‹¨ ê¸°ì¤€)", 0.05, 0.95, 0.35, step=0.05)
input_lang     = st.sidebar.selectbox("ì…ë ¥ ì–¸ì–´", ["auto", "ko", "en"], index=0)
force_call     = st.sidebar.checkbox("ìœ„í—˜í•´ë„ GPT í˜¸ì¶œ ê°•í–‰", value=False)

st.sidebar.caption(f"HF: {REPO_ID} ({REPO_TYPE}) / {FILENAME}")
if st.sidebar.button("HF ì—°ê²° ì ê²€"):
    try:
        p = get_ckpt_path()
        size_mb = os.path.getsize(p) / 1_048_576
        st.sidebar.success(f"OK: {os.path.basename(p)} Â· {size_mb:.1f} MB")
    except Exception as e:
        st.sidebar.error("ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨")
        st.sidebar.exception(e)

txt = st.text_area("í”„ë¡¬í”„íŠ¸", height=140, placeholder="ì˜ˆ) ì¸ì²œ ë§›ì§‘ ì•Œë ¤ì¤˜")
run = st.button("ë¶„ì„ (GPT í˜¸ì¶œ)")

if run:
    with st.spinner("ë¶„ì„ ì¤‘..."):
        result = predict(txt, thr_ui=thr_ui)
    st.success(f"ë¶„ì„ ì™„ë£Œ ({result['_elapsed_s']:.2f}s)")
    st.subheader("ë¶„ì„ ê²°ê³¼  â†ªï¸")
    st.json({k: v for k, v in result.items() if not k.startswith("_")})

    st.subheader("GPT ì‘ë‹µ")
    if not OPENAI_API_KEY:
        st.info("OPENAI_API_KEYê°€ ì—†ì–´ GPT í˜¸ì¶œì€ ìƒëµí–ˆìŠµë‹ˆë‹¤.")
    elif result["íŒì •"] == "ì•…ì„±" and not force_call:
        st.warning("ì•…ì„±ìœ¼ë¡œ íŒì •ë˜ì–´ GPT í˜¸ì¶œì„ ì°¨ë‹¨í–ˆìŠµë‹ˆë‹¤. ('ê°•í–‰' ì˜µì…˜ìœ¼ë¡œ ë¬´ì‹œ ê°€ëŠ¥)")
    else:
        try:
            from openai import OpenAI
            client = OpenAI(api_key=OPENAI_API_KEY)
            sys_prompt = (
                "You are a helpful assistant. If the user's prompt is clearly harmful or illegal, "
                "politely refuse and provide safe alternatives."
            )
            msgs = [{"role": "system", "content": sys_prompt},
                    {"role": "user", "content": txt}]
            rsp = client.chat.completions.create(
                model=openai_model,
                messages=msgs,
                temperature=0.3,
                top_p=1.0,
            )
            st.write(rsp.choices[0].message.content.strip())
        except Exception as e:
            st.error(f"GPT í˜¸ì¶œ ì˜¤ë¥˜: {type(e).__name__}: {e}")
            st.caption("429(ì¿¼í„° ì´ˆê³¼) ë“± ìš”ê¸ˆì œ/ëª¨ë¸ ì´ë¦„ì„ í™•ì¸í•˜ì„¸ìš”.")
