# app.py â”€ KillSwitch AI â€” Streamlit ë°ëª¨
import os, time, re, json
import streamlit as st

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 0) Hugging Face ì²´í¬í¬ì¸íŠ¸ ë¡œë” (í—ˆë¸Œâ†’ë¡œì»¬ ìºì‹œ)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import torch
from huggingface_hub import hf_hub_download
from transformers import AutoTokenizer, AutoModelForSequenceClassification

# â˜…â˜… ì—¬ê¸°ë§Œ ë„ˆì˜ í—ˆë¸Œ ë¦¬í¬/íŒŒì¼ë¡œ ìˆ˜ì • â˜…â˜…
REPO_ID   = "your-username/killswitch-ai-checkpoints"   # ì˜ˆ: "beomsu/killswitch-ckpt"
REPO_TYPE = "model"                                     # ëª¨ë¸ ë¦¬í¬ë©´ "model", ë°ì´í„°ì…‹ì´ë©´ "dataset"
FILENAME  = "pt/prompt_guard_best.pt"                   # í—ˆë¸Œ ë‚´ë¶€ ê²½ë¡œ(í´ë”/íŒŒì¼)

@st.cache_resource(show_spinner=False)
def get_ckpt_path():
    # 1) í”„ë¡œì íŠ¸ì— íŒŒì¼ì„ ê°™ì´ ë„£ì—ˆìœ¼ë©´ ë¡œì»¬ ìš°ì„ 
    local_path = os.path.join("model", "prompt_guard_best.pt")
    if os.path.exists(local_path):
        return local_path
    # 2) í—ˆë¸Œì—ì„œ ë‹¨ì¼ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
    token = st.secrets.get("HF_TOKEN") or os.getenv("HF_TOKEN")
    return hf_hub_download(repo_id=REPO_ID, filename=FILENAME,
                           repo_type=REPO_TYPE, token=token)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1) ëª¨ë¸/í† í¬ë‚˜ì´ì € ë¡œë“œ
#   - HF_DIR í™˜ê²½ë³€ìˆ˜ë¡œ from_pretrained ë””ë ‰í† ë¦¬ë¥¼ ì§ì ‘ ì§€ì •í•  ìˆ˜ë„ ìˆìŒ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
BASE_MODEL = "microsoft/deberta-v3-base"  # ì‹¤ì œ í•™ìŠµì— ì“´ ë² ì´ìŠ¤ë¡œ ë³€ê²½ ê°€ëŠ¥
NUM_LABELS = 2

@st.cache_resource(show_spinner=False)
def load_model_tokenizer():
    # tokenizer: sentencepiece í•„ìš” â†’ requirements.txtì— í¬í•¨
    tok = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=False)

    # ìš°ì„ ìˆœìœ„ 1) ì™„ì „í•œ HF í˜•ì‹ ë””ë ‰í† ë¦¬(HF_DIR)
    hf_dir = os.getenv("HF_DIR")
    if hf_dir and os.path.isdir(hf_dir):
        mdl = AutoModelForSequenceClassification.from_pretrained(hf_dir)
        mdl.eval()
        return mdl, tok, 0.5, True  # thr ê¸°ë³¸ 0.5, torch_loaded=True

    # ìš°ì„ ìˆœìœ„ 2) BASE_MODEL ë¡œë“œ í›„ state_dict ë®ì–´ì“°ê¸°
    mdl = AutoModelForSequenceClassification.from_pretrained(BASE_MODEL, num_labels=NUM_LABELS)
    thr = 0.5
    torch_loaded = False
    try:
        ckpt_path = get_ckpt_path()
        ckpt = torch.load(ckpt_path, map_location="cpu", weights_only=False)
        state = ckpt["model"] if isinstance(ckpt, dict) and "model" in ckpt else ckpt
        mdl.load_state_dict(state, strict=False)
        thr = float(ckpt.get("val_thr", 0.5)) if isinstance(ckpt, dict) else 0.5
        torch_loaded = True
    except Exception as e:
        st.info("í•™ìŠµ ì²´í¬í¬ì¸íŠ¸ë¥¼ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤(ê·œì¹™ ê¸°ë°˜ë§Œ ì‚¬ìš©). ìƒì„¸:")
        st.caption(str(e))
    mdl.eval()
    return mdl, tok, thr, torch_loaded

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2) ê°„ë‹¨ ê·œì¹™ ê¸°ë°˜ ì ìˆ˜ (í‚¤ì›Œë“œ ë§¤ì¹­)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
BAD_PATTERNS = [
    r"ìì‚´|ìí•´|ê·¹ë‹¨ì \s*ì„ íƒ", r"í­íƒ„|ì´ê¸°|ì‚´ì¸|í…ŒëŸ¬", r"ë§ˆì•½|í•„ë¡œí°|ì½”ì¹´ì¸",
    r"í•´í‚¹|ë””ë„ìŠ¤|ëœì„¬ì›¨ì–´|ì·¨ì•½ì \s*ì•…ìš©", r"ë¹„ë°€ë²ˆí˜¸|íŒ¨ìŠ¤ì›Œë“œ|OTP|ë°±ë„ì–´",
    r"ì£¼ë¯¼ë“±ë¡ë²ˆí˜¸|ì—¬ê¶Œë²ˆí˜¸|ì‹ ìš©ì¹´ë“œ\s*ë²ˆí˜¸"
]
BAD_RE = re.compile("|".join(BAD_PATTERNS), re.I)

def rule_detect(text: str):
    if not text.strip():
        return 0.0, []
    found = sorted(set([m.group(0) for m in BAD_RE.finditer(text)]))
    score = min(1.0, len(found) * 0.4) if found else 0.0  # í‚¤ì›Œë“œë‹¹ ê°€ì¤‘ì¹˜
    return score, found

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3) ëª¨ë¸ ì¶”ë¡  + ìœµí•© ë¡œì§ (rule â¨‚ model)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_resource(show_spinner=False)
def _cached_mdl():
    return load_model_tokenizer()

def predict(text: str, thr_ui: float):
    mdl, tok, thr_ckpt, torch_loaded = _cached_mdl()
    t0 = time.time()

    # â‘  ê·œì¹™ ì ìˆ˜
    r_score, r_keys = rule_detect(text)

    # â‘¡ ëª¨ë¸ ì ìˆ˜(ìˆìœ¼ë©´)
    m_score = 0.0
    if torch_loaded:
        enc = tok(text, return_tensors="pt", truncation=True, padding=True, max_length=256)
        with torch.no_grad():
            logits = mdl(**enc).logits
            if logits.size(-1) == 1:
                m_score = torch.sigmoid(logits)[0, 0].item()
            else:
                m_score = torch.softmax(logits, dim=-1)[0, 1].item()

    # â‘¢ ìœµí•©(ê°„ë‹¨: max) â€” ë³´ìˆ˜ì ìœ¼ë¡œ ë§‰ê¸°
    score = max(r_score, m_score)
    thr = float(thr_ui if thr_ui is not None else thr_ckpt)
    label = "ì•…ì„±" if score >= thr else "ì•ˆì „"

    elapsed = time.time() - t0
    return {
        "ì ìˆ˜": round(score, 3),
        "ì„ê³„ê°’": round(thr, 3),
        "íŒì •": label,
        "í‚¤ì›Œë“œ": r_keys or ["-"],
        "ì„¸ë¶€": {
            "rule_score": round(r_score, 3),
            "model_score": round(m_score, 3),
            "torch_loaded": bool(torch_loaded),
        },
        "_elapsed_s": round(elapsed, 2),
    }

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4) UI
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(page_title="KillSwitch AI â€” Streamlit ë°ëª¨", layout="wide")
st.title("ğŸ›¡ï¸ KillSwitch AI â€” Streamlit ë°ëª¨")

# â”€ Sidebar: ì„¤ì •
st.sidebar.header("ì„¤ì •")
OPENAI_API_KEY = st.sidebar.text_input("OPENAI_API_KEY", type="password")
openai_model   = st.sidebar.text_input("OpenAI ëª¨ë¸", value="gpt-5") 
thr_ui         = st.sidebar.slider("ì„ê³„ê°’(ì°¨ë‹¨ ê¸°ì¤€)", 0.05, 0.95, 0.70, step=0.05)
input_lang     = st.sidebar.selectbox("ì…ë ¥ ì–¸ì–´", ["auto", "ko", "en"], index=0)
force_call     = st.sidebar.checkbox("ìœ„í—˜í•´ë„ GPT í˜¸ì¶œ ê°•í–‰", value=False)

# â”€ Main: ì…ë ¥ & ë²„íŠ¼
txt = st.text_area("í”„ë¡¬í”„íŠ¸", height=140, placeholder="ì˜ˆ) ì¸ì²œ ë§›ì§‘ ì•Œë ¤ì¤˜")
col1, col2 = st.columns([1,1])
btn_analyze = col1.button("ë¶„ì„")
btn_both    = col2.button("ë¶„ì„ í›„ GPT í˜¸ì¶œ")

# â”€ ë¶„ì„
result = None
if btn_analyze or btn_both:
    result = predict(txt, thr_ui=thr_ui)
    st.success(f"ë¶„ì„ ì™„ë£Œ ({result['_elapsed_s']:.2f}s)")
    st.subheader("ë¶„ì„ ê²°ê³¼  â†ªï¸")
    st.json({k: v for k, v in result.items() if not k.startswith("_")})

# â”€ GPT í˜¸ì¶œ
st.subheader("GPT ì‘ë‹µ")
if btn_both:
    if not OPENAI_API_KEY:
        st.error("OPENAI_API_KEYê°€ ë¹„ì—ˆìŠµë‹ˆë‹¤. ì‚¬ì´ë“œë°”ì— ì…ë ¥í•´ì£¼ì„¸ìš”.")
    elif result and result["íŒì •"] == "ì•…ì„±" and not force_call:
        st.warning("ì•…ì„±ìœ¼ë¡œ íŒì •ë˜ì–´ GPT í˜¸ì¶œì„ ì°¨ë‹¨í–ˆìŠµë‹ˆë‹¤. (ì‚¬ì´ë“œë°”ì—ì„œ 'ê°•í–‰' ì²´í¬ ì‹œ ì‹œë„)")
    else:
        try:
            # OpenAI Chat Completions (python SDK v1)
            from openai import OpenAI
            client = OpenAI(api_key=OPENAI_API_KEY)
            # ê°„ë‹¨í•œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ â€” ì•ˆì „ ê°€ì´ë“œ ìœ ì§€(ìš°íšŒ ì§€ì‹œ X)
            sys_prompt = (
                "You are a helpful assistant. If the user's prompt is clearly harmful or illegal, "
                "politely refuse and provide safe alternatives."
            )
            msgs = [{"role": "system", "content": sys_prompt},
                    {"role": "user", "content": txt}]

            rsp = client.chat.completions.create(
                model=openai_model,
                messages=msgs,
                temperature=0.3,
                top_p=1.0,
            )
            st.write(rsp.choices[0].message.content.strip())
        except Exception as e:
            st.error(f"GPT í˜¸ì¶œ ì˜¤ë¥˜: {type(e).__name__}: {e}")
            st.caption("ìš”ê¸ˆì œ/ì¿¼í„° ë¶€ì¡±(429)ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê²°ì œ/ëª¨ë¸ ì´ë¦„ì„ í™•ì¸í•˜ì„¸ìš”.")

# â”€ í•˜ë‹¨ ë„ì›€ë§
st.markdown("---")
st.caption("HF_DIR í™˜ê²½ë³€ìˆ˜ë¡œ í•™ìŠµ ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ ì§€ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. (ì˜ˆ: models/hf_checkpoint)")
st.caption(f"í—ˆë¸Œ: {REPO_ID} / íŒŒì¼: {FILENAME} / RepoType: {REPO_TYPE}")
