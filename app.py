# app.py â€” KillSwitch AI Â· Streamlit ë°ëª¨ (ì„¸ì…˜ ìœ ì§€ íŒ¨ì¹˜ ì ìš©)
# ----------------------------------------------------------------------
# âœ” HF ì²´í¬í¬ì¸íŠ¸ ë¡œë”©
# âœ” ê·œì¹™(rule) + ëª¨ë¸ ì ìˆ˜ ìœµí•©
# âœ” ì‚¬ì´ë“œë°” OPENAI_API_KEY ì…ë ¥(ì„¸ì…˜ ìœ ì§€)
# âœ” í•„ìš” ì‹œ GPT í˜¸ì¶œ
# ----------------------------------------------------------------------

import os, time, re, json
import streamlit as st

# 0) í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="KillSwitch AI", layout="wide")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1) í™˜ê²½/ì‹œí¬ë¦¿ ì„¤ì •
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DEFAULT_REPO_ID   = "your-username/killswitch-ai-checkpoints"
DEFAULT_REPO_TYPE = "model"
DEFAULT_FILENAME  = "pt/prompt_guard_best.pt"

REPO_ID   = st.secrets.get("HF_REPO_ID")   or os.getenv("HF_REPO_ID")   or DEFAULT_REPO_ID
REPO_TYPE = st.secrets.get("HF_REPO_TYPE") or os.getenv("HF_REPO_TYPE") or DEFAULT_REPO_TYPE
FILENAME  = st.secrets.get("HF_FILENAME")  or os.getenv("HF_FILENAME")  or DEFAULT_FILENAME
HF_TOKEN  = st.secrets.get("HF_TOKEN")     or os.getenv("HF_TOKEN")
HF_DIR    = st.secrets.get("HF_DIR")       or os.getenv("HF_DIR")

BASE_MODEL = os.getenv("BASE_MODEL") or "microsoft/deberta-v3-base"
NUM_LABELS = int(os.getenv("NUM_LABELS") or 2)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2) HF ì²´í¬í¬ì¸íŠ¸ & ëª¨ë¸ ë¡œë“œ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import torch
from huggingface_hub import hf_hub_download
from transformers import AutoTokenizer, AutoModelForSequenceClassification

@st.cache_resource(show_spinner=False)
def get_ckpt_path() -> str:
    local = os.path.join("model", "prompt_guard_best.pt")
    if os.path.exists(local):
        return local
    try:
        return hf_hub_download(repo_id=REPO_ID, filename=FILENAME,
                               repo_type=REPO_TYPE, token=HF_TOKEN)
    except:
        alt = "dataset" if REPO_TYPE == "model" else "model"
        return hf_hub_download(repo_id=REPO_ID, filename=FILENAME,
                               repo_type=alt, token=HF_TOKEN)

@st.cache_resource(show_spinner=False)
def load_model_tokenizer():
    try:
        tok = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=False)
    except:
        st.info("fast tokenizer ë¡œ ì¬ì‹œë„")
        tok = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)

    if HF_DIR and os.path.isdir(HF_DIR):
        mdl = AutoModelForSequenceClassification.from_pretrained(HF_DIR)
        mdl.eval()
        return mdl, tok, 0.5, True

    mdl = AutoModelForSequenceClassification.from_pretrained(BASE_MODEL, num_labels=NUM_LABELS)
    thr = 0.5
    torch_loaded = False
    try:
        ckpt_path = get_ckpt_path()
        ckpt = torch.load(ckpt_path, map_location="cpu", weights_only=False)
        state = ckpt["model"] if isinstance(ckpt, dict) and "model" in ckpt else ckpt
        mdl.load_state_dict(state, strict=False)
        thr = float(ckpt.get("val_thr", 0.5)) if isinstance(ckpt, dict) else 0.5
        torch_loaded = True
    except Exception as e:
        st.info("ì²´í¬í¬ì¸íŠ¸ ë¯¸ì ìš©(ê·œì¹™ë§Œ)")
        st.caption(str(e))

    mdl.eval()
    return mdl, tok, thr, torch_loaded

_cached_model = st.cache_resource(show_spinner=False)(load_model_tokenizer)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3) ê·œì¹™ ê¸°ë°˜ íƒì§€
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
BAD_PATTERNS = [
    r"ìì‚´|ìí•´|ê·¹ë‹¨ì \s*ì„ íƒ", r"í­íƒ„|ì´ê¸°|ì‚´ì¸|í…ŒëŸ¬",
    r"ë§ˆì•½|í•„ë¡œí°|ì½”ì¹´ì¸", r"í•´í‚¹|ë””ë„ìŠ¤|ëœì„¬ì›¨ì–´|ì·¨ì•½ì ",
    r"ë¹„ë°€ë²ˆí˜¸|OTP|ë°±ë„ì–´|ì£¼ë¯¼ë“±ë¡ë²ˆí˜¸|ì‹ ìš©ì¹´ë“œ",
    r"í”¼ì‹±|phishing",
    r"ì”¨ë°œ|ã……ã…‚|ê°œìƒˆë¼|ë³‘ì‹ |ì§€ë„",
    r"kill|murder"
]
BAD_RE = re.compile("|".join(BAD_PATTERNS), re.I)

def rule_detect(text: str):
    found = sorted(set([m.group(0) for m in BAD_RE.finditer(text)]))
    score = min(1.0, len(found)*0.4) if found else 0.0
    return score, found

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4) ì¶”ë¡  & ìœµí•©
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def predict(text: str, thr_ui: float):
    mdl, tok, thr_ckpt, torch_loaded = _cached_model()
    r_score, r_keys = rule_detect(text)

    m_score = 0.0
    if torch_loaded:
        enc = tok(text, return_tensors="pt", truncation=True, padding=True, max_length=256)
        with torch.no_grad():
            logits = mdl(**enc).logits
            if logits.size(-1)==1:
                m_score = torch.sigmoid(logits)[0,0].item()
            else:
                m_score = torch.softmax(logits, dim=-1)[0,1].item()

    score = max(r_score, m_score)
    thr = float(thr_ui if thr_ui is not None else thr_ckpt)
    label = "ì•…ì„±" if score >= thr else "ì•ˆì „"

    return {
        "ì ìˆ˜": round(score,3),
        "ì„ê³„ê°’": round(thr,3),
        "íŒì •": label,
        "í‚¤ì›Œë“œ": r_keys or ["-"],
        "ì„¸ë¶€": {
            "rule_score": round(r_score,3),
            "model_score": round(m_score,3),
            "torch_loaded": torch_loaded,
        }
    }

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 5) UI (OPENAI_API_KEY ì„¸ì…˜ ìœ ì§€)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.title("ğŸ›¡ï¸ KillSwitch AI")

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "OPENAI_API_KEY" not in st.session_state:
    st.session_state.OPENAI_API_KEY = ""

OPENAI_API_KEY = st.sidebar.text_input(
    "OPENAI_API_KEY", 
    value=st.session_state.OPENAI_API_KEY,
    type="password"
)
if OPENAI_API_KEY != st.session_state.OPENAI_API_KEY:
    st.session_state.OPENAI_API_KEY = OPENAI_API_KEY

openai_model   = st.sidebar.text_input("OpenAI ëª¨ë¸", value="gpt-4o-mini")
thr_ui         = st.sidebar.slider("ì„ê³„ê°’(ì°¨ë‹¨ ê¸°ì¤€)", 0.05, 0.95, 0.35, step=0.05)
force_call     = st.sidebar.checkbox("ìœ„í—˜í•´ë„ GPT í˜¸ì¶œ ê°•í–‰", value=False)

st.sidebar.caption(f"HF: {REPO_ID} ({REPO_TYPE}) / {FILENAME}")

txt = st.text_area("í”„ë¡¬í”„íŠ¸", height=140, placeholder="ì˜ˆ) ì¸ì²œ ë§›ì§‘ ì•Œë ¤ì¤˜")
run = st.button("ë¶„ì„ (GPT í˜¸ì¶œ)")

if run:
    result = predict(txt, thr_ui)
    st.subheader("ë¶„ì„ ê²°ê³¼  â†ªï¸")
    st.json(result)

    st.subheader("GPT ì‘ë‹µ")
    if not st.session_state.OPENAI_API_KEY:
        st.info("OPENAI_API_KEYê°€ ì—†ì–´ GPT í˜¸ì¶œ ìƒëµ")
    elif result["íŒì •"] == "ì•…ì„±" and not force_call:
        st.warning("ì•…ì„± íŒì • â†’ GPT í˜¸ì¶œ ì°¨ë‹¨ (ê°•í–‰ ON ì‹œ ê°€ëŠ¥)")
    else:
        try:
            from openai import OpenAI
            client = OpenAI(api_key=st.session_state.OPENAI_API_KEY)
            rsp = client.responses.create(
                model=openai_model,
                input=[
                    {"role": "system", "content": "If harmful, refuse."},
                    {"role": "user", "content": txt}
                ],
            )
            st.write(rsp.output_text)
        except Exception as e:
            st.error(f"GPT í˜¸ì¶œ ì˜¤ë¥˜: {type(e).__name__}: {e}")
