# app.py â€” KillSwitch AI (ìµœì¢…ë³¸)
# - ëª¨ë¸ ì ìˆ˜ + ë©”íƒ€ê°€ì¤‘ì¹˜(ì‹¤í–‰/ì„¤ëª…/ìœ„í—˜ë‹¨ì–´) + softmax í™•ì‹ ë„(gap) ê°ì‡ 
# - GPT ì‘ë‹µ UI ìƒë‹¨ ë°°ì¹˜ / ë‹¨ì¼ ë²„íŠ¼(ë¶„ì„)
# - ì†ë„ ìµœì í™”: fast tokenizer, FP16(CUDA), max_length ë””í´íŠ¸ 224
# --------------------------------------------------------------------------------
import os, re, time, unicodedata
import streamlit as st

os.environ.setdefault("OMP_NUM_THREADS", "4")
os.environ.setdefault("MKL_NUM_THREADS", "4")

st.set_page_config(page_title="KillSwitch AI", layout="wide")

# ===== 1) í™˜ê²½/ì‹œí¬ë¦¿ =====
DEFAULT_REPO_ID   = "cookiechips/KillSwitch_ai"
DEFAULT_REPO_TYPE = "model"
DEFAULT_FILENAME  = "prompt_guard_best.pt"

REPO_ID   = st.secrets.get("HF_REPO_ID")   or os.getenv("HF_REPO_ID")   or DEFAULT_REPO_ID
REPO_TYPE = st.secrets.get("HF_REPO_TYPE") or os.getenv("HF_REPO_TYPE") or DEFAULT_REPO_TYPE
FILENAME  = st.secrets.get("HF_FILENAME")  or os.getenv("HF_FILENAME")  or DEFAULT_FILENAME
HF_TOKEN  = st.secrets.get("HF_TOKEN")     or os.getenv("HF_TOKEN")
HF_DIR    = st.secrets.get("HF_DIR")       or os.getenv("HF_DIR")

BASE_MODEL = "microsoft/deberta-v3-base"
NUM_LABELS = 2

W_ACTION = 0.15
W_INFO   = -0.15
W_DOMAIN = 0.25
GAP_THR  = 0.10
W_UNCERT = -0.10

# ===== 2) ëª¨ë¸ ë¡œë”© =====
import torch
from huggingface_hub import hf_hub_download
from transformers import AutoTokenizer, AutoModelForSequenceClassification

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
torch.set_num_threads(4)

@st.cache_resource
def get_ckpt_path():
    local = os.path.join("model", FILENAME)
    if os.path.exists(local):
        return local
    try:
        return hf_hub_download(REPO_ID, FILENAME, REPO_TYPE, HF_TOKEN)
    except:
        alt = "dataset" if REPO_TYPE=="model" else "model"
        return hf_hub_download(REPO_ID, FILENAME, alt, HF_TOKEN)

@st.cache_resource
def load_model_tokenizer():
    tok = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)
    mdl = AutoModelForSequenceClassification.from_pretrained(BASE_MODEL, num_labels=NUM_LABELS)
    thr = 0.5

    try:
        ckpt = torch.load(get_ckpt_path(), map_location="cpu", weights_only=False)
        state = ckpt["model"] if isinstance(ckpt, dict) and "model" in ckpt else ckpt
        mdl.load_state_dict(state, strict=False)
        if isinstance(ckpt, dict) and "val_thr" in ckpt:
            thr = float(ckpt["val_thr"])
    except Exception as e:
        st.error(f"ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì˜¤ë¥˜: {e}")

    mdl.to(DEVICE).eval()
    if DEVICE.type == "cuda":
        mdl.half()

    return mdl, tok, thr

_cached_model = load_model_tokenizer

# ===== 3) ì „ì²˜ë¦¬ & ëª¨ë¸ ì¶”ë¡  =====
def preprocess(s):
    s = unicodedata.normalize("NFKC", s or "")
    s = re.sub(r"\s+", " ", s).strip()
    return s if s.endswith(".") else s+"."

def _softmax_two(logits):
    probs = torch.softmax(logits, dim=-1)[0]
    p0, p1 = probs[0], probs[1]
    gap = abs(p1 - p0)
    return p0.item(), p1.item(), gap.item()

@torch.inference_mode()
def model_forward(mdl, tok, text, max_len):
    enc = tok(text, return_tensors="pt", truncation=True, padding=False, max_length=max_len)
    seq_len = enc["input_ids"].shape[-1]
    for k in enc: enc[k] = enc[k].to(DEVICE)
    out = mdl(**enc)
    p0, p1, gap = _softmax_two(out.logits)
    return p1, gap, seq_len

# ===== 4) ë©”íƒ€ ê°€ì¤‘ì¹˜ =====
ACTION_PAT = re.compile(r"(í•´ì¤˜|ë§Œë“¤ì–´|êµ¬í˜„|ì½”ë“œ|ìŠ¤í¬ë¦½íŠ¸|ìë™í™”|ì‹¤í–‰|ê³µê²©|ë‹¤ìš´ë¡œë“œ|install|inject|exploit)", re.IGNORECASE)
INFO_PAT = re.compile(r"(ì˜ë¯¸|ì •ì˜|ì›ë¦¬|ì´ìœ |ì„¤ëª…|ì•Œë ¤ì¤˜|ìš”ì•½|ê°€ì´ë“œ|ë¬´ì—‡)", re.IGNORECASE)
DANGER_WORDS = ["í­íƒ„","ddos","í•´í‚¹","ë¬´ê¸°","ëœì„¬ì›¨ì–´","ë°±ë„ì–´","ì•…ì„±ì½”ë“œ","í”¼ì‹±","cvv","ë¹„ë°€ë²ˆí˜¸","íƒˆì·¨"]

def detect_meta(t):
    t=t.lower()
    return bool(ACTION_PAT.search(t)), bool(INFO_PAT.search(t)), any(w in t for w in DANGER_WORDS)

def apply_weights(p1,g,a,i,d):
    score=p1
    if a: score+=W_ACTION
    if i: score+=W_INFO
    if d: score+=W_DOMAIN
    if g<GAP_THR: score+=W_UNCERT
    return max(0,min(1,score))

# ===== 5) ì˜ˆì¸¡ íŒŒì´í”„ë¼ì¸ =====
def predict(text, thr_ui, max_len):
    text = preprocess(text)
    mdl,tok,thr = _cached_model()
    t0=time.time()

    p1,gap,seq_len = model_forward(mdl,tok,text,max_len)
    act,info,danger = detect_meta(text)
    adj = apply_weights(p1,gap,act,info,danger)
    label = "ì•…ì„±" if adj>=thr else "ì•ˆì „"

    return {
        "íŒì •":label, "ì›ì ìˆ˜":round(p1,3),"ì¡°ì •":round(adj,3),
        "ì„ê³„":thr, "gap":round(gap,3),
        "act":act,"info":info,"danger":danger,
        "seq_len":seq_len,"max_len":max_len,
        "_elapsed":round(time.time()-t0,2)
    }

# ===== 6) UI =====
st.title("ğŸ›¡ï¸ KillSwitch AI")

if "OPENAI_API_KEY" not in st.session_state:
    st.session_state.OPENAI_API_KEY=""

OPENAI_API_KEY = st.sidebar.text_input("OPENAI_API_KEY",st.session_state.OPENAI_API_KEY,type="password")
st.session_state.OPENAI_API_KEY=OPENAI_API_KEY

thr_ui = st.sidebar.slider("ì„ê³„ê°’",0.05,0.95,0.70,0.05)
max_len_ui = st.sidebar.slider("max_length",128,256,224,32)
force_call = st.sidebar.checkbox("ìœ„í—˜í•´ë„ GPT ê°•í–‰",False)
openai_model = st.sidebar.text_input("GPT ëª¨ë¸","gpt-4o-mini")

txt = st.text_area("í”„ë¡¬í”„íŠ¸",height=140,placeholder="ì˜ˆ) ì¸ì²œ ë§›ì§‘ ì•Œë ¤ì¤˜")

@st.cache_resource
def get_openai_client():
    from openai import OpenAI
    k=st.session_state.OPENAI_API_KEY or os.getenv("OPENAI_API_KEY")
    return OpenAI(api_key=k) if k else None

if st.button("ë¶„ì„"):
    if not txt.strip():
        st.warning("í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
    else:
        with st.spinner("ë¶„ì„ ì¤‘..."):
            r=predict(txt,thr_ui,max_len_ui)

        st.success(f"ë¶„ì„ ì™„ë£Œ ({r['_elapsed']}s)")
        st.markdown("### âœ… ìš”ì•½")

        c1,c2,c3,c4,c5=st.columns([1.2,1,1,1,1])
        with c1: st.metric("íŒì •",r["íŒì •"])
        with c2: st.metric("ì›ì ìˆ˜",r["ì›ì ìˆ˜"])
        with c3: st.metric("ì¡°ì •",r["ì¡°ì •"])
        with c4: st.metric("ì„ê³„",r["ì„ê³„"])
        with c5: st.metric("í™•ì‹ ë„",r["gap"])

        if r["seq_len"]>=r["max_len"]:
            st.caption("âš ï¸ ì…ë ¥ì´ ë„ˆë¬´ ê¸¸ì–´ ì¼ë¶€ê°€ ì˜ë ¸ì„ ìˆ˜ ìˆì–´ìš”.")

        st.markdown("### ğŸ¤– GPT ì‘ë‹µ")
        client=get_openai_client()

        if client is None:
            st.info("ğŸ”‘ API Key ì—†ìŒ â†’ GPT ë¯¸í˜¸ì¶œ")
        elif r["íŒì •"]=="ì•…ì„±" and not force_call:
            st.warning("ğŸ›‘ ì•…ì„± â†’ GPT ì°¨ë‹¨(ê°•í–‰ ì˜µì…˜ í™œì„± ì‹œ ê°€ëŠ¥)")
        else:
            try:
                rsp=client.responses.create(
                    model=openai_model,
                    input=[
                        {"role":"system","content":"ë¶ˆë²•/ìœ í•´ ìš”ì²­ì€ ì •ì¤‘íˆ ê±°ì ˆí•˜ê³  ì•ˆì „í•œ ë°©í–¥ ì•ˆë‚´"},
                        {"role":"user","content":txt},
                    ],
                    temperature=0.3
                )
                st.write(rsp.output_text)
            except Exception as e:
                st.error(f"GPT ì˜¤ë¥˜: {e}")

        with st.expander("ğŸ” ì„¸ë¶€/ê·¼ê±°"):
            st.write({
                "ì‹¤í–‰í˜•":r["act"],"ì„¤ëª…í˜•":r["info"],
                "ìœ„í—˜ë‹¨ì–´":r["danger"],
                "seq_len":r["seq_len"],"max_len":r["max_len"]
            })
