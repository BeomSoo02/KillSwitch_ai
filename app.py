# app.py â€” KillSwitch AI (ëª¨ë¸ ì ìˆ˜ë§Œ + Robust Scoring + í‚¤ ìƒíƒœ + ì§„ë‹¨ ìœ í‹¸ Â· í’€ë²„ì „)
# --------------------------------------------------------------------------------------
# âœ” HF ì²´í¬í¬ì¸íŠ¸ ë¡œë”©(state_dict) + ì—°ê²° ì ê²€ ë²„íŠ¼(íŒŒì¼ í¬ê¸° í‘œì‹œ)
# âœ” ëª¨ë¸ ì ìˆ˜ë§Œ ì‚¬ìš©(ê·œì¹™/í‚¤ì›Œë“œ ì œê±°)
# âœ” ë§ˆì¹¨í‘œ ê°•ê±´í™”(ì›ë³¸/ì œê±°/ì¶”ê°€) + mean/max ì„ íƒ
# âœ” ì‚¬ì´ë“œë°” OPENAI_API_KEY ì…ë ¥(ì„¸ì…˜ ìœ ì§€, password) + ğŸ”í‚¤ ìƒíƒœ í‘œì‹œ
# âœ” ì¶”ë¡  ì‹œê°„/ë””ë°”ì´ìŠ¤/í† í¬ë‚˜ì´ì € íƒ€ì… í‘œì‹œ
# âœ” í•„ìš” ì‹œ GPT í˜¸ì¶œ(Responses API)
# --------------------------------------------------------------------------------------

import os, re, time, unicodedata
import streamlit as st

# ========== 0) í˜ì´ì§€ ì„¤ì • ==========
st.set_page_config(page_title="KillSwitch AI", layout="wide")

# ========== 1) í™˜ê²½/ì‹œí¬ë¦¿ ê¸°ë³¸ê°’ ==========
DEFAULT_REPO_ID   = "your-username/killswitch-ai-checkpoints"
DEFAULT_REPO_TYPE = "model"                      # or "dataset"
DEFAULT_FILENAME  = "pt/prompt_guard_best.pt"    # í—ˆë¸Œ ë‚´ ê²½ë¡œ/íŒŒì¼ëª…

REPO_ID   = st.secrets.get("HF_REPO_ID")   or os.getenv("HF_REPO_ID")   or DEFAULT_REPO_ID
REPO_TYPE = st.secrets.get("HF_REPO_TYPE") or os.getenv("HF_REPO_TYPE") or DEFAULT_REPO_TYPE
FILENAME  = st.secrets.get("HF_FILENAME")  or os.getenv("HF_FILENAME")  or DEFAULT_FILENAME
HF_TOKEN  = st.secrets.get("HF_TOKEN")     or os.getenv("HF_TOKEN")     # ë¹„ê³µê°œ ë¦¬í¬ë©´ í•„ìš”
HF_DIR    = st.secrets.get("HF_DIR")       or os.getenv("HF_DIR")       # ì™„ì „ ëª¨ë¸ ë””ë ‰í† ë¦¬(ì˜µì…˜)

BASE_MODEL = os.getenv("BASE_MODEL") or "microsoft/deberta-v3-base"
NUM_LABELS = int(os.getenv("NUM_LABELS") or 2)

# ========== 2) ëª¨ë¸/í—ˆë¸Œ ë¡œë”© ==========
import torch
from huggingface_hub import hf_hub_download
from transformers import AutoTokenizer, AutoModelForSequenceClassification

# ë””ë°”ì´ìŠ¤ ì •ë³´(ì§„ë‹¨ìš©)
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

@st.cache_resource(show_spinner=False)
def get_ckpt_path() -> str:
    """
    í—ˆë¸Œì—ì„œ .pt ì²´í¬í¬ì¸íŠ¸ë¥¼ ë°›ì•„ ìºì‹œì— ì €ì¥í•˜ê³  íŒŒì¼ ê²½ë¡œë¥¼ ë°˜í™˜.
    ./model/prompt_guard_best.pt ê°€ ë¡œì»¬ì— ìˆìœ¼ë©´ ê·¸ê±¸ ìš°ì„  ì‚¬ìš©.
    """
    local = os.path.join("model", "prompt_guard_best.pt")
    if os.path.exists(local):
        return local

    # ì„ ì–¸ëœ repo_typeìœ¼ë¡œ ì‹œë„ â†’ ì‹¤íŒ¨ ì‹œ ë°˜ëŒ€ íƒ€ì…ìœ¼ë¡œ ì¬ì‹œë„
    try:
        return hf_hub_download(
            repo_id=REPO_ID,
            filename=FILENAME,
            repo_type=REPO_TYPE,
            token=HF_TOKEN
        )
    except Exception:
        alt = "dataset" if REPO_TYPE == "model" else "model"
        return hf_hub_download(
            repo_id=REPO_ID,
            filename=FILENAME,
            repo_type=alt,
            token=HF_TOKEN
        )

@st.cache_resource(show_spinner=False)
def load_model_tokenizer():
    """
    1) HF_DIR ì§€ì • ì‹œ ì™„ì „ ëª¨ë¸ ë””ë ‰í† ë¦¬ì—ì„œ from_pretrained
    2) ì•„ë‹ˆë©´ BASE_MODEL ë¡œë“œ í›„ .pt state_dict ì£¼ì…
    ë°˜í™˜: (model, tokenizer, thr, torch_loaded, tok_info)
    """
    # í† í¬ë‚˜ì´ì €: sentencepiece í•„ìš”ë¡œ slow ìš°ì„  â†’ ì‹¤íŒ¨ ì‹œ fast
    tok_info = "slow"
    try:
        tok = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=False)
    except Exception:
        tok = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)
        tok_info = "fast"

    # 1) ì™„ì „ ëª¨ë¸ ë””ë ‰í† ë¦¬
    if HF_DIR and os.path.isdir(HF_DIR):
        mdl = AutoModelForSequenceClassification.from_pretrained(HF_DIR)
        mdl.to(DEVICE).eval()
        return mdl, tok, 0.5, True, tok_info

    # 2) ë² ì´ìŠ¤ + state_dict
    mdl = AutoModelForSequenceClassification.from_pretrained(BASE_MODEL, num_labels=NUM_LABELS)
    thr = 0.5
    torch_loaded = False
    try:
        ckpt_path = get_ckpt_path()
        ckpt = torch.load(ckpt_path, map_location="cpu", weights_only=False)
        state = ckpt["model"] if isinstance(ckpt, dict) and "model" in ckpt else ckpt

        # ì™„ì „ ì¼ì¹˜ê°€ ì•„ë‹ˆì–´ë„ ì¼ë‹¨ ì£¼ì…(strict=False)
        missing, unexpected = mdl.load_state_dict(state, strict=False)
        if missing or unexpected:
            st.caption(f"state_dict mismatch â†’ missing:{len(missing)}, unexpected:{len(unexpected)}")

        # ì²´í¬í¬ì¸íŠ¸ì— ì„ê³„ê°’ ì €ì¥ë¼ ìˆìœ¼ë©´ ìš°ì„ 
        try:
            if isinstance(ckpt, dict) and "val_thr" in ckpt:
                thr = float(ckpt["val_thr"])
        except Exception:
            pass

        torch_loaded = True
    except Exception as e:
        st.info("ì²´í¬í¬ì¸íŠ¸ë¥¼ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤(ëª¨ë¸ ë¯¸ë¡œë”©).")
        st.caption(str(e))

    mdl.to(DEVICE).eval()
    return mdl, tok, thr, torch_loaded, tok_info

_cached_model = st.cache_resource(show_spinner=False)(load_model_tokenizer)

# ========== 3) ì „ì²˜ë¦¬/ìŠ¤ì½”ì–´ë§(robust) ==========
def preprocess(s: str) -> str:
    """NFKC ì •ê·œí™” + ê³µë°± ì •ë¦¬"""
    s = unicodedata.normalize("NFKC", s or "")
    s = re.sub(r"\s+", " ", s).strip()
    return s

@torch.no_grad()
def score_once(mdl, tok, text: str) -> float:
    """í•œ ë²ˆë§Œ ì ìˆ˜: ì‹œê·¸ëª¨ì´ë“œ(1í—¤ë“œ) or ì†Œí”„íŠ¸ë§¥ìŠ¤(2+í—¤ë“œ)"""
    enc = tok(text, return_tensors="pt", truncation=True, padding=True, max_length=256)
    for k in enc:
        enc[k] = enc[k].to(DEVICE)
    logits = mdl(**enc).logits
    if logits.size(-1) == 1:
        return torch.sigmoid(logits)[0, 0].item()
    return torch.softmax(logits, dim=-1)[0, 1].item()

def robust_score(mdl, tok, text: str, method: str = "mean") -> float:
    """
    ë§ˆì¹¨í‘œ ê°•ê±´í™”: ì›ë¬¸ / ëë§ˆì¹¨í‘œ ì œê±° / ëë§ˆì¹¨í‘œ ê°•ì œ ì¶”ê°€ 3íšŒ ìŠ¤ì½”ì–´ë§.
    method="mean" â†’ í‰ê·  / "max" â†’ ìµœëŒ“ê°’
    """
    v1 = text
    v2 = text.rstrip(". ")
    v3 = (text.rstrip() + ".")
    scores = [score_once(mdl, tok, v) for v in (v1, v2, v3)]
    return max(scores) if method == "max" else sum(scores) / len(scores)

def predict(text: str, thr_ui: float, robust_method: str):
    """
    ëª¨ë¸ ì ìˆ˜ë§Œìœ¼ë¡œ íŒì • (ê·œì¹™/í‚¤ì›Œë“œ ì œê±°)
    robust_method: "mean" or "max"
    """
    text = preprocess(text)
    mdl, tok, thr_ckpt, torch_loaded, tok_info = _cached_model()
    t0 = time.time()

    m_score = robust_score(mdl, tok, text, method=robust_method) if torch_loaded else 0.0
    thr = float(thr_ui if thr_ui is not None else thr_ckpt)
    label = "ì•…ì„±" if m_score >= thr else "ì•ˆì „"

    return {
        "ì ìˆ˜": round(m_score, 3),
        "ì„ê³„ê°’": round(thr, 3),
        "íŒì •": label,
        "ì„¸ë¶€": {
            "model_score": round(m_score, 3),
            "torch_loaded": bool(torch_loaded),
            "robust_method": robust_method,
            "device": str(DEVICE),
            "tokenizer": tok_info,
        },
        "_elapsed_s": round(time.time() - t0, 2),
    }

# ========== 4) UI ==========
st.title("ğŸ›¡ï¸ KillSwitch AI")

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”(í‚¤)
if "OPENAI_API_KEY" not in st.session_state:
    st.session_state.OPENAI_API_KEY = ""

# --- ì‚¬ì´ë“œë°”: í‚¤ ìƒíƒœ í‘œì‹œ ---
with st.sidebar.expander("ğŸ” í‚¤ ìƒíƒœ"):
    key_from_secrets = bool(st.secrets.get("OPENAI_API_KEY"))
    key_from_env     = bool(os.getenv("OPENAI_API_KEY"))
    key_from_session = bool(st.session_state.get("OPENAI_API_KEY"))
    key_ok = key_from_secrets or key_from_env or key_from_session

    st.write("OpenAI Key:", "âœ… ê°ì§€ë¨" if key_ok else "âŒ ì—†ìŒ")
    st.caption(
        f"â€¢ Secrets: {'âœ…' if key_from_secrets else 'â€”'}   "
        f"â€¢ Env: {'âœ…' if key_from_env else 'â€”'}   "
        f"â€¢ Session: {'âœ…' if key_from_session else 'â€”'}"
    )

# --- ì‚¬ì´ë“œë°”: ì˜µì…˜ ---
OPENAI_API_KEY = st.sidebar.text_input(
    "OPENAI_API_KEY",
    value=st.session_state.OPENAI_API_KEY,
    type="password"
)
if OPENAI_API_KEY != st.session_state.OPENAI_API_KEY:
    st.session_state.OPENAI_API_KEY = OPENAI_API_KEY

openai_model   = st.sidebar.text_input("OpenAI ëª¨ë¸", value="gpt-4o-mini")
thr_ui         = st.sidebar.slider("ì„ê³„ê°’(ì°¨ë‹¨ ê¸°ì¤€)", 0.05, 0.95, 0.5, step=0.05)
robust_method  = st.sidebar.radio("ê°•ê±´í™” ë°©ì‹", ["mean", "max"], index=0, horizontal=True)
force_call     = st.sidebar.checkbox("ìœ„í—˜í•´ë„ GPT í˜¸ì¶œ ê°•í–‰", value=False)

# --- ì‚¬ì´ë“œë°”: HF ì—°ê²° ì ê²€ ---
st.sidebar.caption(f"HF: {REPO_ID} ({REPO_TYPE}) / {FILENAME}")
if st.sidebar.button("HF ì—°ê²° ì ê²€"):
    try:
        p = get_ckpt_path()
        size_mb = os.path.getsize(p) / 1_048_576
        st.sidebar.success(f"OK: {os.path.basename(p)} Â· {size_mb:.1f} MB")
    except Exception as e:
        st.sidebar.error("ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨")
        st.sidebar.exception(e)

# ë©”ì¸ ì…ë ¥/ë²„íŠ¼
txt = st.text_area("í”„ë¡¬í”„íŠ¸", height=140, placeholder="ì˜ˆ) ì¸ì²œ ë§›ì§‘ ì•Œë ¤ì¤˜")
run = st.button("ë¶„ì„ (GPT í˜¸ì¶œ)")

# ========== 5) ì‹¤í–‰ ==========
if run:
    if not (txt and txt.strip()):
        st.warning("í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
    else:
        with st.spinner("ë¶„ì„ ì¤‘..."):
            result = predict(txt, thr_ui, robust_method)
        st.success(f"ë¶„ì„ ì™„ë£Œ ({result['_elapsed_s']:.2f}s)")
        st.subheader("ë¶„ì„ ê²°ê³¼  â†ªï¸")
        st.json({k: v for k, v in result.items() if not k.startswith("_")})

        # --- GPT í˜¸ì¶œ ---
        st.subheader("GPT ì‘ë‹µ")
        if not key_ok:
            st.info("OPENAI_API_KEYê°€ ì—†ì–´ GPT í˜¸ì¶œì„ ìƒëµí–ˆìŠµë‹ˆë‹¤.")
        elif result["íŒì •"] == "ì•…ì„±" and not force_call:
            st.warning("ì•…ì„±ìœ¼ë¡œ íŒì •ë˜ì–´ GPT í˜¸ì¶œì´ ì°¨ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤. (ì‚¬ì´ë“œë°” 'ê°•í–‰'ì„ ì²´í¬í•˜ë©´ í˜¸ì¶œ)")
        else:
            try:
                from openai import OpenAI
                api_key = st.secrets.get("OPENAI_API_KEY") or os.getenv("OPENAI_API_KEY") or st.session_state.OPENAI_API_KEY
                client = OpenAI(api_key=api_key)
                rsp = client.responses.create(
                    model=openai_model,
                    input=[
                        {"role": "system",
                         "content": "You are a helpful assistant. If the user's prompt is clearly harmful or illegal, politely refuse and provide safe alternatives."},
                        {"role": "user", "content": txt},
                    ],
                    temperature=0.3,
                    top_p=1.0,
                )
                st.write(rsp.output_text)
            except Exception as e:
                st.error(f"GPT í˜¸ì¶œ ì˜¤ë¥˜: {type(e).__name__}: {e}")
