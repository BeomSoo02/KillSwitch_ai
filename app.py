# app.py â€” KillSwitch AI (ëª¨ë¸ ì ìˆ˜ + ëë§ˆì¹¨í‘œ + ì˜ë„/ìœ„í—˜/í™•ì‹ ë„ ë³´ì •, GPT ë¯¸ì‚¬ìš©)
# -----------------------------------------------------------------------------------
# âœ” ëª¨ë¸ ì ìˆ˜ë§Œ ì‚¬ìš© (ê·œì¹™ ìµœì†Œ: ì ìˆ˜ ë³´ì •ìš© ê°€ë²¼ìš´ íŒ¨í„´ë§Œ)
# âœ” ëë§ˆì¹¨í‘œ ê°•ì œ (ì›ë¬¸ì´ '.'ë¡œ ëë‚˜ë©´ ì¶”ê°€ ì•ˆ í•¨)
# âœ” ì˜ë„(ì„¤ëª…/ì‹¤í–‰) + ìœ„í—˜ ë‹¨ì–´ + í™•ì‹ ë„(ë¶ˆí™•ì‹¤ ì‹œ ê°ì‡ ) ë³´ì •
# âœ” 'ê²€í† í•„ìš”' ì¤‘ê°„ ë°´ë“œ ì œê³µ (ìš´ì˜/ì‹œì—° ì‹ ë¢°ë„ â†‘)
# âœ” HF ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ(Pytorch 2.6 í˜¸í™˜: weights_only=False)
# âœ” í† í¬ë‚˜ì´ì € slow ìš°ì„ â†’ì‹¤íŒ¨ ì‹œ fast
# âœ” HF ì—°ê²° ì ê²€ ë²„íŠ¼
# -----------------------------------------------------------------------------------

import os, re, time, unicodedata
from typing import List, Dict, Any
import streamlit as st

st.set_page_config(page_title="KillSwitch AI", layout="wide")

# ===== 1) í™˜ê²½/ì‹œí¬ë¦¿ =====
DEFAULT_REPO_ID   = "cookiechips/KillSwitch_ai"   # í•„ìš”í•˜ë©´ êµì²´
DEFAULT_REPO_TYPE = "model"
DEFAULT_FILENAME  = "prompt_guard_best.pt"

REPO_ID   = st.secrets.get("HF_REPO_ID")   or os.getenv("HF_REPO_ID")   or DEFAULT_REPO_ID
REPO_TYPE = st.secrets.get("HF_REPO_TYPE") or os.getenv("HF_REPO_TYPE") or DEFAULT_REPO_TYPE
FILENAME  = st.secrets.get("HF_FILENAME")  or os.getenv("HF_FILENAME")  or DEFAULT_FILENAME
HF_TOKEN  = st.secrets.get("HF_TOKEN")     or os.getenv("HF_TOKEN")
HF_DIR    = st.secrets.get("HF_DIR")       or os.getenv("HF_DIR")

BASE_MODEL = os.getenv("BASE_MODEL") or "microsoft/deberta-v3-base"
NUM_LABELS = int(os.getenv("NUM_LABELS") or 2)

# ===== 2) ëª¨ë¸/í—ˆë¸Œ ë¡œë”© =====
import torch
from huggingface_hub import hf_hub_download
from transformers import AutoTokenizer, AutoModelForSequenceClassification

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

@st.cache_resource(show_spinner=False)
def get_ckpt_path() -> str:
    """í—ˆë¸Œì—ì„œ .pt ì²´í¬í¬ì¸íŠ¸ ë‹¤ìš´ë¡œë“œ (repo_type ì‹¤íŒ¨ ì‹œ ë°˜ëŒ€ íƒ€ìž… ìž¬ì‹œë„)."""
    local = os.path.join("model", FILENAME)
    if os.path.exists(local):
        return local
    try:
        return hf_hub_download(repo_id=REPO_ID, filename=FILENAME, repo_type=REPO_TYPE, token=HF_TOKEN)
    except Exception:
        alt = "dataset" if REPO_TYPE == "model" else "model"
        return hf_hub_download(repo_id=REPO_ID, filename=FILENAME, repo_type=alt, token=HF_TOKEN)

@st.cache_resource(show_spinner=False)
def load_model_tokenizer():
    # í† í¬ë‚˜ì´ì €: slow ìš°ì„  â†’ ì‹¤íŒ¨ ì‹œ fast
    try:
        tok = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=False)
        tok_info = "slow"
    except Exception:
        tok = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)
        tok_info = "fast"

    # ì™„ì „ ëª¨ë¸ ë””ë ‰í† ë¦¬ ìš°ì„ 
    if HF_DIR and os.path.isdir(HF_DIR):
        mdl = AutoModelForSequenceClassification.from_pretrained(HF_DIR)
        mdl.to(DEVICE).eval()
        return mdl, tok, 0.5, True, tok_info

    # ë² ì´ìŠ¤ + state_dict ì£¼ìž…
    mdl = AutoModelForSequenceClassification.from_pretrained(BASE_MODEL, num_labels=NUM_LABELS)
    thr = 0.5
    torch_loaded = False
    try:
        ckpt_path = get_ckpt_path()
        # PyTorch 2.6+ í˜¸í™˜: weights_only=False ëª…ì‹œ
        ckpt = torch.load(ckpt_path, map_location="cpu", weights_only=False)
        state = ckpt["model"] if isinstance(ckpt, dict) and "model" in ckpt else ckpt
        missing, unexpected = mdl.load_state_dict(state, strict=False)
        if missing or unexpected:
            st.caption(f"state_dict mismatch â†’ missing:{len(missing)}, unexpected:{len(unexpected)}")
        if isinstance(ckpt, dict) and "val_thr" in ckpt:
            try:
                thr = float(ckpt["val_thr"])
            except Exception:
                pass
        torch_loaded = True
    except Exception as e:
        st.error("ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì‹¤íŒ¨ â€” ëª¨ë¸ ë¯¸ë¡œë”©")
        st.caption(str(e))

    mdl.to(DEVICE).eval()
    return mdl, tok, thr, torch_loaded, tok_info

_cached_model = st.cache_resource(show_spinner=False)(load_model_tokenizer)

# ===== 3) ì „ì²˜ë¦¬ & ê¸°ë³¸ ì ìˆ˜ =====
def preprocess(s: str) -> str:
    """NFKC + ê³µë°± ì •ë¦¬ + ëë§ˆì¹¨í‘œ ê°•ì œ(ì´ë¯¸ ìžˆìœ¼ë©´ ì¶”ê°€í•˜ì§€ ì•ŠìŒ)."""
    s = unicodedata.normalize("NFKC", s or "")
    s = re.sub(r"\s+", " ", s).strip()
    return s if s.endswith(".") else (s + ".")

@torch.no_grad()
def score_once(mdl, tok, text: str) -> float:
    enc = tok(text, return_tensors="pt", truncation=True, padding=True, max_length=256)
    for k in enc:
        enc[k] = enc[k].to(DEVICE)
    logits = mdl(**enc).logits
    if logits.size(-1) == 1:   # ì‹œê·¸ëª¨ì´ë“œ í—¤ë“œ
        return torch.sigmoid(logits)[0, 0].item()
    return torch.softmax(logits, dim=-1)[0, 1].item()  # ì†Œí”„íŠ¸ë§¥ìŠ¤ 1(ì•…ì„±) í™•ë¥ 

# ===== 4) ì ìˆ˜ ë³´ì •(ì˜ë„/ìœ„í—˜/í™•ì‹ ë„) =====
HIGH_RISK_KEYWORDS = [
    r"í­íƒ„", r"í­ë°œë¬¼", r"í­ë°œ", r"ë¬´ê¸°", r"í…ŒëŸ¬",
    r"ë””ë„ìŠ¤", r"\bDDoS\b", r"ëžœì„¬ì›¨ì–´", r"ì‚´ì¸", r"ì‚´í•´",
    r"\bbomb\b", r"explosive", r"weapon", r"kill\W"
]
ACTION_PATTERNS = [
    r"ë§Œë“¤ì–´ì¤˜", r"ë§Œë“œëŠ”\s*ë°©ë²•", r"ì‹¤í–‰í•´ì¤˜", r"ìž‘ì„±í•´ì¤˜",
    r"ìŠ¤í¬ë¦½íŠ¸\s*ë§Œë“¤ì–´", r"how to", r"\bmake\b", r"\bbuild\b"
]
INFO_PATTERNS = [
    r"ì„¤ëª…í•´ì¤˜", r"ë¬´ì—‡ì¸ì§€", r"ê°œë…", r"ì •ì˜", r"ì´ë¡ ì ìœ¼ë¡œ",
    r"what is", r"\bexplain\b", r"\bdefinition\b"
]

def any_match(patterns: List[str], text: str) -> bool:
    return any(re.search(p, text, re.I) for p in patterns)

def uncertainty_from_prob(p: float) -> float:
    """p=0/1ì—ì„œ 0, p=0.5ì—ì„œ 1 â†’ 0.75 ì´ìƒì´ë©´ ë¶ˆí™•ì‹¤ë¡œ ê°„ì£¼."""
    return 1.0 - abs(p - 0.5) * 2.0

def clamp01(x: float) -> float:
    return max(0.0, min(1.0, x))

def post_adjust(original_text: str, base_prob: float,
                action_boost: float = 0.15,
                info_penalty: float = -0.15,
                highrisk_boost: float = 0.25,
                uncertainty_penalty: float = -0.10) -> (float, float, list):
    """
    base_probì— ë³´ì •ì¹˜ í•©ì‚° â†’ ìµœì¢… ì ìˆ˜ì™€ ë³´ì • í•©(adj), ì´ìœ  ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
    """
    reasons = []
    adj = 0.0

    if any_match(HIGH_RISK_KEYWORDS, original_text):
        adj += highrisk_boost; reasons.append("high-risk-keyword")
    if any_match(ACTION_PATTERNS, original_text):
        adj += action_boost; reasons.append("action-verb")
    if any_match(INFO_PATTERNS, original_text):
        adj += info_penalty; reasons.append("info-verb")

    unc = uncertainty_from_prob(base_prob)
    if unc > 0.75:
        adj += uncertainty_penalty; reasons.append("low-confidence")

    final = clamp01(base_prob + adj)
    return final, adj, reasons

# ===== 5) ì˜ˆì¸¡ íŒŒì´í”„ë¼ì¸ =====
def predict_with_postproc(text: str, ui_thr: float,
                          low_review: float = 0.40, high_review: float = 0.70) -> Dict[str, Any]:
    """
    ìµœì¢… íŒì •:
      - final_score >= ui_thr  â†’ 'ì•…ì„±'
      - low_review <= score < high_review â†’ 'ê²€í† í•„ìš”'
      - else â†’ 'ì•ˆì „'
    """
    mdl, tok, thr_ckpt, torch_loaded, tok_info = _cached_model()
    t0 = time.time()
    txt_proc = preprocess(text)

    base = score_once(mdl, tok, txt_proc) if torch_loaded else 0.0
    final, adj, reasons = post_adjust(text, base)

    if final >= ui_thr:
        label, base_reason = "ì•…ì„±", "model-high"
    elif low_review <= final < high_review:
        label, base_reason = "ê²€í† í•„ìš”", "model-mid"
    else:
        label, base_reason = "ì•ˆì „", "model-low"

    if not reasons:
        reasons = [base_reason]

    return {
        "ì ìˆ˜": round(final, 3),
        "ì›ì ìˆ˜": round(base, 3),
        "ìž„ê³„ê°’": round(ui_thr, 3),
        "íŒì •": label,
        "ê·¼ê±°": reasons,
        "ì„¸ë¶€": {
            "ë³´ì •í•©": round(adj, 3),
            "torch_loaded": bool(torch_loaded),
            "device": str(DEVICE),
            "tokenizer": tok_info,
        },
        "_elapsed_s": round(time.time() - t0, 2),
    }

# ===== 6) UI =====
st.title("ðŸ›¡ï¸ KillSwitch AI â€” Heuristic Post-Processing (No GPT)")

# ì‚¬ì´ë“œë°” ì˜µì…˜
thr_ui      = st.sidebar.slider("ìž„ê³„ê°’(ì•…ì„± íŒì • ì»·)", 0.30, 0.95, 0.50, step=0.05)
low_review  = st.sidebar.slider("ê²€í† í•„ìš”(í•˜í•œ)",        0.10, 0.90, 0.40, step=0.05)
high_review = st.sidebar.slider("ê²€í† í•„ìš”(ìƒí•œ)",        0.20, 0.95, 0.70, step=0.05)

# HF ì—°ê²° ì ê²€
st.sidebar.caption(f"HF: {REPO_ID} ({REPO_TYPE}) / {FILENAME}")
if st.sidebar.button("HF ì—°ê²° ì ê²€"):
    try:
        p = get_ckpt_path()
        size_mb = os.path.getsize(p) / 1_048_576
        st.sidebar.success(f"OK: {os.path.basename(p)} Â· {size_mb:.1f} MB")
    except Exception as e:
        st.sidebar.error("ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨")
        st.sidebar.exception(e)

# ìž…ë ¥ & ì‹¤í–‰
txt = st.text_area("í”„ë¡¬í”„íŠ¸", height=140, placeholder="ì˜ˆ) ì¸ì²œ ë§›ì§‘ ì•Œë ¤ì¤˜")
if st.button("ë¶„ì„ ì‹¤í–‰"):
    if not (txt and txt.strip()):
        st.warning("í…ìŠ¤íŠ¸ë¥¼ ìž…ë ¥í•˜ì„¸ìš”.")
    else:
        with st.spinner("ë¶„ì„ ì¤‘..."):
            result = predict_with_postproc(txt, thr_ui, low_review, high_review)
        st.success(f"ë¶„ì„ ì™„ë£Œ ({result['_elapsed_s']:.2f}s)")
        st.subheader("ë¶„ì„ ê²°ê³¼  â†ªï¸")
        # íŒì • ì´ëª¨ì§€
        emoji = "âœ…" if result["íŒì •"] == "ì•ˆì „" else ("ðŸŸ¡" if result["íŒì •"] == "ê²€í† í•„ìš”" else "â›”")
        st.write(f"**íŒì •:** {result['íŒì •']} {emoji}")
        st.write(f"**ì ìˆ˜:** {result['ì ìˆ˜']} (ì›ì ìˆ˜: {result['ì›ì ìˆ˜']}, ìž„ê³„ê°’: {result['ìž„ê³„ê°’']})")
        st.write(f"**ê·¼ê±°:** {', '.join(result['ê·¼ê±°'])}")
        st.json({k: v for k, v in result.items() if not k.startswith("_")})
