# app.py â€” KillSwitch AI (ëª¨ë¸ ì ìˆ˜ë§Œ + robust mean + ì´ì¤‘ ì„ê³„ê°’ + LLM ë””ëª¨ì…˜ ì˜µì…˜)
# ------------------------------------------------------------------------------------

import os, re, time, unicodedata
from typing import Optional
import streamlit as st

st.set_page_config(page_title="KillSwitch AI", layout="wide")

# ===== 1) í™˜ê²½/ì‹œí¬ë¦¿ =====
DEFAULT_REPO_ID   = "cookiechips/KillSwitch_ai"   # í•„ìš”ì‹œ êµì²´
DEFAULT_REPO_TYPE = "model"
DEFAULT_FILENAME  = "prompt_guard_best.pt"

REPO_ID   = st.secrets.get("HF_REPO_ID")   or os.getenv("HF_REPO_ID")   or DEFAULT_REPO_ID
REPO_TYPE = st.secrets.get("HF_REPO_TYPE") or os.getenv("HF_REPO_TYPE") or DEFAULT_REPO_TYPE
FILENAME  = st.secrets.get("HF_FILENAME")  or os.getenv("HF_FILENAME")  or DEFAULT_FILENAME
HF_TOKEN  = st.secrets.get("HF_TOKEN")     or os.getenv("HF_TOKEN")
HF_DIR    = st.secrets.get("HF_DIR")       or os.getenv("HF_DIR")

BASE_MODEL = os.getenv("BASE_MODEL") or "microsoft/deberta-v3-base"
NUM_LABELS = int(os.getenv("NUM_LABELS") or 2)

# ===== 2) ëª¨ë¸/í—ˆë¸Œ ë¡œë”© =====
import torch
from huggingface_hub import hf_hub_download
from transformers import AutoTokenizer, AutoModelForSequenceClassification

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

@st.cache_resource(show_spinner=False)
def get_ckpt_path() -> str:
    # ë¡œì»¬ í¬í•¨ ì‹œ ìš°ì„ 
    local = os.path.join("model", FILENAME)
    if os.path.exists(local):
        return local
    try:
        return hf_hub_download(repo_id=REPO_ID, filename=FILENAME, repo_type=REPO_TYPE, token=HF_TOKEN)
    except Exception:
        alt = "dataset" if REPO_TYPE == "model" else "model"
        return hf_hub_download(repo_id=REPO_ID, filename=FILENAME, repo_type=alt, token=HF_TOKEN)

@st.cache_resource(show_spinner=False)
def load_model_tokenizer():
    # í† í¬ë‚˜ì´ì €: slow ìš°ì„  â†’ ì‹¤íŒ¨ ì‹œ fast
    try:
        tok = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=False)
        tok_info = "slow"
    except Exception:
        tok = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)
        tok_info = "fast"

    # ì™„ì „ ëª¨ë¸ ë””ë ‰í† ë¦¬
    if HF_DIR and os.path.isdir(HF_DIR):
        mdl = AutoModelForSequenceClassification.from_pretrained(HF_DIR)
        mdl.to(DEVICE).eval()
        return mdl, tok, 0.5, True, tok_info

    # ë² ì´ìŠ¤ + state_dict ì£¼ì…
    mdl = AutoModelForSequenceClassification.from_pretrained(BASE_MODEL, num_labels=NUM_LABELS)
    thr = 0.5
    torch_loaded = False
    try:
        ckpt_path = get_ckpt_path()
        # PyTorch 2.6+ í˜¸í™˜: weights_only=False ëª…ì‹œ
        ckpt = torch.load(ckpt_path, map_location="cpu", weights_only=False)
        state = ckpt["model"] if isinstance(ckpt, dict) and "model" in ckpt else ckpt
        missing, unexpected = mdl.load_state_dict(state, strict=False)
        if missing or unexpected:
            st.caption(f"state_dict mismatch â†’ missing:{len(missing)}, unexpected:{len(unexpected)}")
        if isinstance(ckpt, dict) and "val_thr" in ckpt:
            try:
                thr = float(ckpt["val_thr"])
            except Exception:
                pass
        torch_loaded = True
    except Exception as e:
        st.error("ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì‹¤íŒ¨ â€” ëª¨ë¸ ë¯¸ë¡œë”©")
        st.caption(str(e))

    mdl.to(DEVICE).eval()
    return mdl, tok, thr, torch_loaded, tok_info

_cached_model = st.cache_resource(show_spinner=False)(load_model_tokenizer)

# ===== 3) ì „ì²˜ë¦¬ & robust mean ì ìˆ˜ =====
# ê°€ë²¼ìš´ ì •ê·œí™”(ë£° ì•„ë‹˜): zero-width ì œê±°, êµ¬ë‘ì  í‰íƒ„í™”, ê³µë°± ì •ë¦¬
ZWSP = "".join([chr(c) for c in [0x200B,0x200C,0x200D,0xFEFF]])
ZWSP_RE = re.compile(f"[{re.escape(ZWSP)}]")

def normalize_lite(s: str) -> str:
    s = unicodedata.normalize("NFKC", s or "")
    s = ZWSP_RE.sub("", s)
    s = re.sub(r"[Â·â€¢â—]+", ".", s)        # ì  ìœ ì‚¬ë¬¸ì ë‹¨ìˆœí™”
    s = re.sub(r"[!ï¼]{2,}", "!", s)      # ì—°ì† ëŠë‚Œí‘œ ì¶•ì†Œ
    s = re.sub(r"[?ï¼Ÿ]{2,}", "?", s)      # ì—°ì† ë¬¼ìŒí‘œ ì¶•ì†Œ
    s = re.sub(r"\s+", " ", s).strip()
    return s

@torch.no_grad()
def score_once(mdl, tok, text: str) -> float:
    enc = tok(text, return_tensors="pt", truncation=True, padding=True, max_length=256)
    for k in enc:
        enc[k] = enc[k].to(DEVICE)
    logits = mdl(**enc).logits
    if logits.size(-1) == 1:   # ì‹œê·¸ëª¨ì´ë“œ í—¤ë“œ
        return torch.sigmoid(logits)[0, 0].item()
    return torch.softmax(logits, dim=-1)[0, 1].item()  # ì†Œí”„íŠ¸ë§¥ìŠ¤ 1(ì•…ì„±) í™•ë¥ 

def robust_mean_score(mdl, tok, text: str) -> float:
    t = normalize_lite(text)
    v1 = t
    v2 = t.rstrip(". ")
    v3 = (t.rstrip() + ".")
    scores = [score_once(mdl, tok, v) for v in (v1, v2, v3)]
    return sum(scores) / len(scores)

# ===== 4) LLM ì„¸ì»¨ë“œ ì˜¤í”¼ë‹ˆì–¸ (ì¤‘ê°„ëŒ€ì—­ë§Œ, ì˜¤íƒ ë””ëª¨ì…˜ ì „ìš©) =====
def llm_safety_check(text: str, api_key: str, model_name: str = "gpt-4o-mini") -> str:
    """
    ë°˜í™˜: "SAFE" | "UNSAFE" | "UNKNOWN"
    """
    try:
        from openai import OpenAI
        client = OpenAI(api_key=api_key)
        prompt = (
            "Classify the user's message as SAFE or UNSAFE for safety policy.\n"
            "Return exactly one word: SAFE or UNSAFE.\n\n"
            f"User message:\n{text}"
        )
        rsp = client.responses.create(
            model=model_name,
            input=[{"role": "user", "content": prompt}],
            temperature=0.0,
            top_p=1.0,
            max_output_tokens=5,
        )
        out = (rsp.output_text or "").strip().upper()
        if "UNSAFE" in out:
            return "UNSAFE"
        if "SAFE" in out:
            return "SAFE"
        return "UNKNOWN"
    except Exception:
        return "UNKNOWN"

# ===== 5) ìµœì¢… íŒì •: ì´ì¤‘ ì„ê³„ê°’ + LLM(ì˜µì…˜, ë””ëª¨ì…˜ ì „ìš©) =====
LOW_T  = 0.60
HIGH_T = 0.85

def predict_fused(text: str, ui_thr: float, use_llm_demotion: bool,
                  api_key: Optional[str], model_name: str):
    mdl, tok, thr_ckpt, torch_loaded, tok_info = _cached_model()
    t0 = time.time()

    s_local = robust_mean_score(mdl, tok, text) if torch_loaded else 0.0

    # ê³ ë“ì : ì¦‰ì‹œ ì•…ì„±
    if s_local >= HIGH_T:
        return {
            "ì ìˆ˜": round(s_local,3),
            "ì„ê³„ê°’": round(ui_thr,3),
            "íŒì •": "ì•…ì„±",
            "ê·¼ê±°": "local-high",
            "ì„¸ë¶€": {"torch_loaded": bool(torch_loaded), "device": str(DEVICE), "tokenizer": tok_info},
            "_elapsed_s": round(time.time() - t0, 2),
        }

    # ì €ë“ì : ì¦‰ì‹œ ì•ˆì „
    if s_local < LOW_T:
        return {
            "ì ìˆ˜": round(s_local,3),
            "ì„ê³„ê°’": round(ui_thr,3),
            "íŒì •": "ì•ˆì „",
            "ê·¼ê±°": "local-low",
            "ì„¸ë¶€": {"torch_loaded": bool(torch_loaded), "device": str(DEVICE), "tokenizer": tok_info},
            "_elapsed_s": round(time.time() - t0, 2),
        }

    # ì¤‘ê°„ëŒ€ì—­: LLM ë””ëª¨ì…˜(ì˜¤íƒ ì¤„ì´ê¸°) â€” SAFEë©´ ì•ˆì „ìœ¼ë¡œ ë‚´ë¦¼
    verdict = "skipped"
    if use_llm_demotion and api_key:
        v = llm_safety_check(text, api_key, model_name)
        verdict = v.lower()
        if v == "SAFE":
            return {
                "ì ìˆ˜": round(s_local,3),
                "ì„ê³„ê°’": round(ui_thr,3),
                "íŒì •": "ì•ˆì „",
                "ê·¼ê±°": "llm-safe",
                "ì„¸ë¶€": {"torch_loaded": bool(torch_loaded), "device": str(DEVICE), "tokenizer": tok_info},
                "_elapsed_s": round(time.time() - t0, 2),
            }

    # LLM ë¯¸ì‚¬ìš©/UNKNOWN/UNSAFE â†’ ë¡œì»¬ ì„ê³„ê°’ìœ¼ë¡œ ìµœì¢… íŒì •
    final = "ì•…ì„±" if s_local >= ui_thr else "ì•ˆì „"
    return {
        "ì ìˆ˜": round(s_local,3),
        "ì„ê³„ê°’": round(ui_thr,3),
        "íŒì •": final,
        "ê·¼ê±°": verdict if verdict != "skipped" else "local-mid",
        "ì„¸ë¶€": {"torch_loaded": bool(torch_loaded), "device": str(DEVICE), "tokenizer": tok_info},
        "_elapsed_s": round(time.time() - t0, 2),
    }

# ===== 6) UI =====
st.title("ğŸ›¡ï¸ KillSwitch AI")

# ì„¸ì…˜ ìƒíƒœ(í‚¤)
if "OPENAI_API_KEY" not in st.session_state:
    st.session_state.OPENAI_API_KEY = ""

# ğŸ” í‚¤ ìƒíƒœ
with st.sidebar.expander("ğŸ” í‚¤ ìƒíƒœ"):
    key_from_secrets = bool(st.secrets.get("OPENAI_API_KEY"))
    key_from_env     = bool(os.getenv("OPENAI_API_KEY"))
    key_from_session = bool(st.session_state.get("OPENAI_API_KEY"))
    key_ok = key_from_secrets or key_from_env or key_from_session
    st.write("OpenAI Key:", "âœ… ê°ì§€ë¨" if key_ok else "âŒ ì—†ìŒ")
    st.caption(
        f"â€¢ Secrets: {'âœ…' if key_from_secrets else 'â€”'}   "
        f"â€¢ Env: {'âœ…' if key_from_env else 'â€”'}   "
        f"â€¢ Session: {'âœ…' if key_from_session else 'â€”'}"
    )

# ì‚¬ì´ë“œë°” ì˜µì…˜
OPENAI_API_KEY = st.sidebar.text_input(
    "OPENAI_API_KEY",
    value=st.session_state.OPENAI_API_KEY,
    type="password"
)
if OPENAI_API_KEY != st.session_state.OPENAI_API_KEY:
    st.session_state.OPENAI_API_KEY = OPENAI_API_KEY

thr_ui            = st.sidebar.slider("ì„ê³„ê°’(ìµœì¢… íŒì •)", 0.50, 0.95, 0.75, step=0.05)
use_llm_demotion  = st.sidebar.checkbox("ì˜¤íƒ ì¤„ì´ê¸°: LLM ì„¸ì»¨ë“œ ì˜¤í”¼ë‹ˆì–¸(ì¤‘ê°„ëŒ€ì—­ë§Œ)", value=True)
llm_model_name    = st.sidebar.text_input("LLM ëª¨ë¸(ë””ëª¨ì…˜ìš©)", value="gpt-4o-mini")
gen_model_answer  = st.sidebar.checkbox("ì›ë¬¸ì— ëŒ€í•œ GPT ë‹µë³€ ìƒì„±", value=False)

# HF ì—°ê²° ì ê²€
st.sidebar.caption(f"HF: {REPO_ID} ({REPO_TYPE}) / {FILENAME}")
if st.sidebar.button("HF ì—°ê²° ì ê²€"):
    try:
        p = get_ckpt_path()
        size_mb = os.path.getsize(p) / 1_048_576
        st.sidebar.success(f"OK: {os.path.basename(p)} Â· {size_mb:.1f} MB")
    except Exception as e:
        st.sidebar.error("ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨")
        st.sidebar.exception(e)

# ë©”ì¸ ì…ë ¥ & ì‹¤í–‰
txt = st.text_area("í”„ë¡¬í”„íŠ¸", height=140, placeholder="ì˜ˆ) ì¸ì²œ ë§›ì§‘ ì•Œë ¤ì¤˜")
if st.button("ë¶„ì„ ì‹¤í–‰"):
    if not (txt and txt.strip()):
        st.warning("í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
    else:
        api_key = (st.secrets.get("OPENAI_API_KEY") or os.getenv("OPENAI_API_KEY") or st.session_state.OPENAI_API_KEY)
        with st.spinner("ë¶„ì„ ì¤‘..."):
            result = predict_fused(txt, thr_ui, use_llm_demotion, api_key if key_ok else None, llm_model_name)
        st.success(f"ë¶„ì„ ì™„ë£Œ ({result['_elapsed_s']:.2f}s)")
        st.subheader("ë¶„ì„ ê²°ê³¼  â†ªï¸")
        st.json({k: v for k, v in result.items() if not k.startswith("_")})

        # (ì˜µì…˜) ì›ë¬¸ì— ëŒ€í•œ GPT ë‹µë³€ ìƒì„±
        if gen_model_answer:
            st.subheader("GPT ì‘ë‹µ")
            if not key_ok:
                st.info("OPENAI_API_KEYê°€ ì—†ì–´ GPT í˜¸ì¶œì„ ìƒëµí–ˆìŠµë‹ˆë‹¤.")
            else:
                try:
                    from openai import OpenAI
                    client = OpenAI(api_key=api_key)
                    rsp = client.responses.create(
                        model=llm_model_name,
                        input=[
                            {"role": "system",
                             "content": "You are a helpful assistant. If the user's prompt is clearly harmful or illegal, politely refuse and provide safe alternatives."},
                            {"role": "user", "content": txt},
                        ],
                        temperature=0.3,
                        top_p=1.0,
                    )
                    st.write(rsp.output_text)
                except Exception as e:
                    st.error(f"GPT í˜¸ì¶œ ì˜¤ë¥˜: {type(e).__name__}: {e}")
